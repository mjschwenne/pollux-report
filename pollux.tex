\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{bbold}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{nicematrix}
\usepackage{mathpartir}
\usepackage{parskip}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usetikzlibrary{positioning,calc,snakes,automata,arrows.meta}
\usepackage{array}
\usepackage{multirow}
\usepackage{listings}
\usepackage{stmaryrd}
\usepackage[backend=biber,sorting=ynt]{biblatex}
\addbibresource{pollux.bib}
\author{Matt Schwennesen}
\title{Pollux: Protobuf Compatibility Checking}
\date{}
\hypersetup{
 pdfauthor={Matt Schwennesen},
 pdftitle={Pollux},
 pdfkeywords={},
 pdfsubject={},
 pdflang={English}
}

\lstdefinelanguage{proto}{
  keywords={message,double,float,int32,int64,uint32,uint64,sint32,sint64,fixed32,fixed64,sfixed32,sfixed64,bool,string,bytes,oneof,enum}
}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{pollux}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=pollux}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\newcommand{\fstar}{F$^\star$}
\renewcommand{\emptyset}{\varnothing}
\renewcommand{\mod}{\;\%\;}
\newcommand{\imp}[1]{\mathtt{IMP}\ #1}
\newcommand{\opt}[1]{\mathtt{OPT}\ #1}
\newcommand{\rep}[1]{\mathtt{REP}\ #1}
\newcommand{\some}[1]{\mathtt{Some}\ #1}
\newcommand{\none}{\mathtt{NONE}}
\newcommand{\field}[3]{\mathtt{FIELD}\ #1\ #2\ #3}
\newcommand{\map}[4]{\mathtt{MAP}\ #1\ #2\ #3\ #4}
\newcommand{\oneof}[2]{\mathtt{ONEOF}\ #1\ #2}
\newcommand{\msg}[3]{\mathtt{MSG}\ #1\ #2\ #3}
\newcommand{\enum}[2]{\mathtt{ENUM}\ #1\ #2}

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}

Software has become an integral and integrated part of everyday life, and with
that comes a near constant stream of updates to various devices all around
us. Unfortunately, software updates are a frequent source of issues
\autocites{zhangUnderstandingDetectingSoftware2021}[][]{Gray1986WhyDC}, often
making it into production environments before the necessary edge cases are
triggered. We believe that techniques from formal verification can be applied
here to ensure update compatibility, which is ultimately what my project aims to
do. Unlike some previous work
\autocites{ajmaniModularSoftwareUpgrades2006}[][]{reitblattAbstractionsNetworkUpdate2012},
I aim to prove compatibility of software updates without imposing restrictions
about how the update itself is performed.

The most nebulous part of this goal is how to define ``compatible'', which I
break down into two major categories; data compatibility and operational
compatibility. Data compatibility aims to show that that the new version can
correctly interpret persistent state left behind by the pervious version while
operational compatibility reasons about behavior of invoking the same function
or feature in both versions of the software. This project is currently working
on data compatibility.

Imagine that you're a developer for a distributed system using \texttt{protobuf}
and \texttt{grpc} to communicate between nodes. A new version of
\texttt{protobuf} just came out and added a syntax for maps to the
protocol. When you used to need maps, you had to manually encode a list of
key-value pairs but now you can write \texttt{map<string, int32>} in the
\texttt{.proto} schema file. Is changing between these syntactic definitions a
compatible update? From a high level, they both model the same mathematical map,
which is the information the program is actually trying to communicate, so it
should be possible to define a compatible update this way. The specific answer
naturally depends on how the new \texttt{map} is encoded into binary, and in the
case of \texttt{protobuf} a \texttt{map} is syntactic sugar for a list of
key-value pairs so this is a valid update\footnote{There is slightly more to it
	than that, regarding some \texttt{optional} tags which might not present in
	the original definition but it is possible to define this update in a
	compatible way}. While this example is considering a rolling update to a
distributed system, the same question would apply to an application which writes
the serialized blob to disk and reads it when the application starts up again.

The notion of both types of maps encoding the same mathematical map leads us to
the idea of abstract state. When we're programming, we don't often think about
which bucket an element of a hashmap is in but rather we think at a higher
level. It is possible to conceptualize this formally, were we have some abstract
state and a concrete state which are linked with a predicate asserting that the
abstract state is captured by the concrete state. This is the same relationship
we see in proofs of functional correctness, were the specification of a function
corresponds to the abstract state and the implementation to the concrete
state. This relationship will serve as the theoretical backing to the project.

More practically, consider an RPC or other serialization library such as
Protobuf or Cap'n'Proto. Updates to RPC schema are an excellent place to start
exploring notions of data compatibility since RPC schema are separated from the
source code that operates on them, are highly structured serialized formats and
are designed to allow the schema to evolve though updates. Tools already exist
which can report on if two versions of a schema file are compatible, these tools
operate using ad-hoc notions of compatibility asserting a specification or
verifying the implementation. Using \fstar{}, I plan to verify a compatibility
checker for one of these libraries before expanding the project to consider a
wider range of data formats. It should also be possible to include some analysis
of the source code operating on the state to prove more invasive updates, or
even the proofs of verified pieces of software, although that is a goal for a
longer timescale.

\section{Protobuf Primer}

Protobuf, or protocol buffers, is a message description language and
serialization protocol developed by Google. It is similar to other descriptive
data formats like ASN1, and is commonly used as part of the gRPC remote
procedure call system. Protobuf and gRPC are one of the most popular
serialization formats.

\subsection{The \texttt{proto} Language}

A high-level protobuf message description is written in a \texttt{.proto} file
in the \texttt{proto} language. Each message is described by a \emph{message
	descriptor} which declares each field and its corresponding type.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 results_per_page = 3;
}\end{lstlisting}

	\caption{Example protobuf message description from the protobuf documentation~\cite{LanguageGuideProto}.}
	\label{fig:proto-ex}
\end{figure}

In figure~\ref{fig:proto-ex}, a message called \texttt{SearchRequest} is
declared. This message contains three fields, the query, page number to display
and number of results per page. Each field is assigned a \emph{field number},
which is used in the serialized version of the message. Each field number must
be unique, as they are used to identify the corresponding binary blob in the
encoded message. A \texttt{.proto} file can define multiple messages, and
messages can be embedded in another by using the name of that message as the
type of a field in the other message.

Each field is defined with a type, with the primitive types summarized in
table~\ref{tbl:proto-prim}.

\begin{table}[H]
	\centering
	\begin{tabular}{cl}
		\toprule
		Type              & Notes                                                    \\
		\midrule
		\texttt{double}   & Standard 64 bit double-precision floating point
		number.                                                                      \\
		\texttt{float}    & Standard 32 bit floating point number.                   \\
		\texttt{int32}    & Variable-length signed 32 bit integer.                   \\
		\texttt{int64}    & Variable-length signed 64 bit integer.                   \\
		\texttt{uint32}   & Variable-length unsigned 32 bit integer.                 \\
		\texttt{uint64}   & Variable-length unsigned 64 bit integer.                 \\
		\texttt{sint32}   & Variable-length 32 bit integer with more
		efficient encoding for negative numbers.                                     \\
		\texttt{sint64}   & Variable-length 64 bit integer with more efficient
		encoding for negative numbers.                                               \\
		\texttt{fixed32}  & Four byte integer, more efficient than \texttt{uint32}
		for values greater than $2^{38}$.                                            \\
		\texttt{fixed64}  & Eight byte integer, more efficient than \texttt{uint64}
		for values greater than $2^{56}$.                                            \\
		\texttt{sfixed32} & Four byte signed integer.                                \\
		\texttt{sfixed64} & Eight byte signed integer.                               \\
		\texttt{bool}     & Boolean, encoded in one byte.                            \\
		\texttt{string}   & A string of UFT-8 or 7-bit ASCII characters shorter than
		$2^{32}$ bytes.                                                              \\
		\texttt{bytes}    & An arbitrary sequence of less than $2^{32}$ bytes.       \\
		\bottomrule
	\end{tabular}
	\caption{Summary of the protobuf scalar values.}
	\label{tbl:proto-prim}
\end{table}

Any protobuf field can marked as \texttt{optional} or \texttt{repeated}. An
\texttt{optional} field can be either set to a value or omitted from the
message. Similarly, a \texttt{repeated} field can be repeated in the message
zero or more times, functioning as a list now. A field without a cardinality
modifier is known as an implicit field and can also be omitted from a message,
just like an \texttt{optional} field. During de-serialization, a type-specific
default value will be used (this is consistent with accessing the value of an
unset \texttt{optional} field). The only difference between an implicit field
and \texttt{optional} field is that the optional field can differentiate between
an unset field and field set to the default value by providing a method which
reports if the \texttt{optional} field is set.

Since the protobuf language is designed to be both forward and backwards
compatible, it is possible to remove or add fields. When removing fields, it is
encouraged to reserve the field number to prevent reuse of the field in a
future. It is also possible to reserved field names, which isn't important for
the protobuf binary encoding format, this is important if you're using the JSON
encoding of a protobuf message.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

message Foo {
  reserved 2, 15, 9 to 11;
  reserved "foo", "bar";
}\end{lstlisting}

	\caption{Example protobuf message using reserved fields~\cite{LanguageGuideProto}.}
	\label{fig:proto-reserved}
\end{figure}

Protobuf also supports enumerations, for when a field may only take a finite
number of values. Due to how default values work in protobuf, enums are required
to have a value defined with a value zero and it should have either
``UNSPECIFIED'' or ``UNKNOWN'' since that will be the default value.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

enum {
  CORPUS_UNSPECIFIED = 0;
  CORPUS_UNIVERSAL = 1;
  CORPUS_WEB = 2;
  CORPUS_IMAGES = 3;
  CORPUS_LOCAL = 4;
  CORPUS_NEWS = 5;
  CORPUS_PRODUCTS = 6;
  CORPUS_VIDEO = 7;
}

message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 results_per_page = 3;
  Corpus corpus = 4;
}\end{lstlisting}

	\caption{Example protobuf message using an enum~\cite{LanguageGuideProto}.}
	\label{fig:proto-enum}
\end{figure}

There are two other features which need to be modeled, \texttt{map} and
\texttt{oneof}. Maps can be thought of like a standard map in \texttt{go} or a
\texttt{dict} in \texttt{python}. From a serialization perspective, this is an
unordered map that could be modeled as a list of key-value pairs. A \texttt{map}
field cannot be \texttt{repeated}, and repeated keys have the last occurrence
win, as is standard with protobuf (see Section~\ref{sec:proto-enc}).

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.43\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
  map<int32, string> map_field = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}[bt]{0.48\textwidth}
		\begin{lstlisting}[language=proto]
message MapFieldEntry {
  int32 key = 1;
  string value = 2;
}

message Foo {
  repeated MapFieldEntry map_field = 1;
}\end{lstlisting}
	\end{minipage}

	\caption{Example of \texttt{map} syntax and the corresponding
		wire-equivalent syntax~\cite{LanguageGuideProto}.}
	\label{fig:proto-map}
\end{figure}

The final important feature of the proto language is a \texttt{oneof} field,
which operates like a union in \texttt{C}. Setting any member of the
\texttt{oneof} will clear any previously set field. A \texttt{oneof} field
cannot contain a \texttt{repeated} field or a \texttt{map} and the
\texttt{oneof} field itself cannot be \texttt{repeated}.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

message User {
  oneof user_id {
    string email = 4;
    int32 phone = 2;
  }
}\end{lstlisting}

	\caption{Example protobuf message using an enum~\cite{LanguageGuideProto}.}
	\label{fig:proto-enum2}
\end{figure}

\subsection{The Protobuf Encoding}\label{sec:proto-enc}

Complimenting the Protobuf schema language is the protobuf encoding format. All
valid protobuf messages written the the proto language will be eventually
encoding into a binary blob in respecting this encoding.

\subsubsection{Variable-Length Integers}

The heart of the protobuf encoding is the variable-length integer encoding,
which represents a 32 or 64 bit integer in between one and ten bytes. The
encoding format itself is relatively simple. The bits of the integer to be
encoded are grouped into bundles of seven. The bundles are ordered in little
endian order with the first bit of each byte set to one if the next byte is also
part of the same integer.

For example, consider the number 163, or 10100011 in binary. Splitting these
into bundles of seven bits produces ``0000001'' and ``0100011''. These are
reordered into little endian order, ``0000001'' then ``0100011''. Finally set
the continuation bit on the first byte of the encoding and concatenate them
together to get ``10000010100011''.

\subsubsection{Field Identifiers}

A message is serialized as a sequence of fields, in any order. Each field is
encoded as a \emph{tag-length value} structure, although not ever field has a
dedicated length in the encoding. The tag consists of the field number as
defined in the \texttt{proto} file, encoded in the variable length integer
encoding discussed above. However, the tag isn't just the field number, it also
includes information on the type of payload as detailed in
Table~\ref{tab:tags}. The wire type is stored in the last three bits of the
field number varint, effectively encoded as \texttt{(field\_number << 3) |
  wire\_type}. 

\subsubsection{Length Delimited Fields}

Nested messages, string and bytes are encoded with their payload length in bytes
as a varint just after the tag. Despite being variable-length, varint fields
aren't encoded with an explicit length since checking the length of the varint
doesn't provide significance benefits when the maximum length varint is only 10
bytes. Lengths are encoded in bytes so that deserializers can skip large unknown
fields efficiently and know when exactly the unknown field ends.

All variable length fields except varints are prefixed with a length and then
the rest of the payload, with nested messages simply beginning with the tag for
their first field and continuing from there. Since nested messages include the
complete field encodings, it is important for the parser to be able to skip an
unknown field exactly rather than incorrectly thinking a nested unknown field is
setting field in the parent message.

\subsubsection{Signed Integers}

The standard varint encoding is only for unsigned integers. Signed integers can
be encoded in both \texttt{int32} and \texttt{int64} or \texttt{sint32} and
\texttt{sint64} field types. The \texttt{intN} types uses standard two's
complement to encode negative integers. However, since a negative two's
compliment integer always has the sign bit set, any negative number will be
encoded in a maximum length varint 10 bytes long.

On the other hand, the \texttt{sintN} types use a ``ZigZag'' encoding to more
efficiently represent smaller negative numbers~\cite{Encoding}. In this
encoding, a positive value $p$ is encoded as $2 \times p$ while a negative value $n$
is encoded as $2 \times |n| - 1$. Using bit-shift operators, this would be
\verb|(n << 1) ^ (n >> 31)| or the naturally extended 64-bit version.

\begin{table}[htbp]
  \centering
  \begin{tabular}{cc}
    \toprule
    Signed Original & Zig-Zag Encoding \\
    \midrule
    0 & 0 \\
    -1 & 1 \\
    1 & 2 \\
    -2 & 3 \\
    \vdots & \vdots \\
    \bottomrule
  \end{tabular}
\caption{Some zig zag encoded integers}
\label{tab:zigzag}
\end{table}

While the zig-zag encoding does save space for smaller negative values in
particular, it does increase the amount of space required to encode large
positive values, which is why it isn't the standard encoding for signed
integers. 

\section{Abstract State Schema}

In software verification, an implementation is always verified against some
specification of correct behavior. This is implicitly part of
\texttt{protobuf} too; when a message is serialized and transmitted over the
network, the developer expects that the same structure can be recreated by the
receiving program. Refactoring this intuition to be more directly applicable to
the standard verification setup, one could argue that the binary wire blob
serves as an ``implementation'' of the structure or object in the host
language (be it \texttt{go}, \texttt{java}, \texttt{python} or any other
language with \texttt{protobuf} bindings) which itself is generated by
\texttt{protoc} and is an ``implementation'' of the proto schema defined in a
\texttt{.proto} file. This stack defines the \texttt{.proto} file as the top
level source of truth, but this can be modeled by the high level host language
in most cases. From the prospective of an application developer, the host
language representation is the most important, since this is where the schema
structure interacts with other parts of the host application.

The definition of a \texttt{proto} message can be modeled by an \fstar{} type,
in a way similar to structures created by \texttt{protoc} for other host
languages. This is not fully sufficient though, since the representation
exposed by \texttt{protoc} isn't enough to fully reason about compatibility.

\subsection{\fstar{} Proto Struct Representation}

Some parts of the translation from a \texttt{proto} message
\autocite{LanguageGuideProto} into \fstar{} is simple. For example, a 64 bit
unsigned integer can be represented with a \texttt{UInt64} from the \fstar{}
machine integer library. Likewise a message can be represented by a record
\autocite{swamy2023proof}. Not everything cleanly generalizes
though. Specifically, it is unclear to how represent some primitive types such
as \texttt{float}, \texttt{double} or \texttt{string}.

\begin{table}[H]
	\centering
	\begin{tabular}{ll}
		\toprule
		\texttt{proto} Feature & \fstar{} Representation   \\
		\midrule
		Integers               & Machine Integer Library   \\
		\texttt{string}        & \texttt{string}           \\
		\texttt{message}       & Record                    \\
		\texttt{optional}      & \texttt{option}           \\
		\texttt{repeated}      & \texttt{list}             \\
		\texttt{map}           & ???                       \\
		\texttt{bool}          & \texttt{bool}             \\
		\texttt{bytes}         & \texttt{Seq.seq UInt8.t}  \\
		\texttt{enum}          & Inductive type            \\
		\texttt{oneof}         & Inductive type (Sum type) \\
		\texttt{option}        & ???                       \\
		\bottomrule
	\end{tabular}

	\vspace{4mm}
	\caption[]{\texttt{proto} language features and corresponding \fstar{} representations.}
\end{table}

Notice that there are several unknown representations in the table. Below is
some commentary on these features:

\begin{itemize}
	\item \texttt{map}: As far as I can tell, \fstar{} doesn't have maps, however
	      this \fstar{} development doesn't \emph{need} maps beyond a way to serialize
	      and de-serialize them. We should be able to develop a parametric
	      \texttt{map} definition as a list of tuples, just like how protobuf would
	      serialize them.
	\item \texttt{option}: This isn't an optional value, but rather top level
	      options used to set things like the \texttt{java} package. At the moment,
	      I'm not sure if or how changing values can impact protobuf compatibility.
\end{itemize}

A good completeness test might be encoding \texttt{descriptor.proto}, the proto
file which can encode other protobuf schema.

While this would provide a framework for building \fstar{} bindings for
protobuf, a richer representation for a protobuf schema for checking
compatibility at the wire level.

\subsection{Compatibility Checking with \fstar{} Types}

When a protobuf message is encoding, field names are stripped out in favor of
field numbers or field tags, which are manually defined in the protobuf
schema. Consider the two \texttt{proto} snippets below:

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    int32 bar = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{1cm}
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    int32 bar = 2;
}\end{lstlisting}
	\end{minipage}

	\caption[]{Example showing the importance of including proto field numbers in
		compatibility checking.}
\end{figure}

Any invocation of \texttt{protoc} will generate exactly the same
application-facing code, even though a message generated using the left proto
file will be parsed to an empty message by the right proto file.

This suggests that a more descriptive type is required. Rather than model the
struct that would be output by \texttt{protoc}, the source type for Pollux
should target the descriptor, as represented in the \texttt{.proto} file. Then,
in a manner similar to the \texttt{parse} function in Everparse, develop
something like this:

\begin{align*}
	\mathtt{Desc}               & : Type                                                                             \\
	\mathtt{Record}             & : Type \quad \text{Record corresponding to descriptor}                             \\
	\llbracket \cdot \rrbracket & : \mathtt{Desc} \rightarrow \mathtt{Record}                                        \\
	parse                       & : (d:\mathtt{Desc}) \rightarrow \mathtt{Bytes} \rightarrow option\
	\llbracket d \rrbracket                                                                                          \\
	serialize                   & : (d:\mathtt{Desc}) \rightarrow \llbracket d \rrbracket \rightarrow \mathtt{Bytes}
\end{align*}

Several shorthands arise from this, namely $parse_d : \mathtt{Bytes} \rightarrow option\
	\llbracket d \rrbracket$ for the partial application of a descriptor $d$ to
$parse$. The same thing can be done for $serialize_d : \llbracket d \rrbracket \rightarrow
	\mathtt{Bytes}$, which naturally to more traditional definitions of a parser and serializer.

\section{Protobuf Compatibility Questions}

As the \fstar{} development moves forward, it is beneficial to have a minimal
working example of a non-trivial compatibility question for protocol
buffers. There are several clearly trivial questions, like updating a
\texttt{int32} to an \texttt{int64} using the variable width integer encoding.

The protobuf encoding format is simple, which makes it challenging to construct
non-trivial compatibility question from small, trivial protobuf descriptions. As
an example, consider this example:

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Bar {
    string baz = 1;
}

message Foo {
    Bar bar = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{1cm}
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    string bar = 1;
}\end{lstlisting}
	\end{minipage}

	\caption[]{A compatibility question.}
\end{figure}

The data is correctly exposed in the string on the right but there will be
several junk bytes at the beginning that the application has to trim. As a
definitional question, should this be considered compatible? Where or not a
client application can detect and correct the contents of the \texttt{bar} field
is beyond the current scope of this project.

From an encoding prospective, a protobuf message is encoded (mostly) as a
sequence of id, tag and value tuples. The id is the field number encoded in the
proto descriptor, the tag marks the type of value to follow and finally the
value contains the content set by the encoding application. More specifically,
the tag is from the table below.

\begin{table}[H]
	\centering
	\begin{tabular}{cll}
		\toprule
		ID & Name            & Uses                                                 \\
		\midrule
		0  & \texttt{VARINT} & \texttt{int32}, \texttt{int64}, \texttt{uint32},
		\texttt{uint64}, \texttt{sint32}, \texttt{sint64},
		\texttt{bool}, \texttt{enum}                                                \\
		1  & \texttt{I64}    & \texttt{fixed64}, \texttt{sfixed64}, \texttt{double} \\
		2  & \texttt{LEN}    & \texttt{string}, \texttt{bytes}, embedded messages,
		packed repeated fields                                                      \\
		5  & \texttt{I32}    & \texttt{fixed32}, \texttt{sfixed32},
		\texttt{float}                                                              \\
		\bottomrule
	\end{tabular}

	\vspace{4mm}
	\caption[]{Description of \texttt{proto} tag values. Note that values 3 and
		4 correspond to \texttt{SGROUP} and \texttt{EGROUP}, tags used in protobuf
		version 2 and deprecated in protobuf 3 \autocite{Encoding}.}\label{tab:tags}
\end{table}

It has finally become time for a sequence of definitions. A protobuf message
descriptor is comprised of a series of field descriptors, each of which has the
form \texttt{<<optional modifiers>> <<type>> <<field name>> = <<field
	number>>;}. During the encoding process, each field is encoded into an
encoding field, represented as tuple with 3 elements.
\\
\begin{definition}[Encoding Field]
	An encoding field is a tuple $(id, tag, value)$ which encodes the field
	number, tag and value of one field of a proto descriptor.
\end{definition}

Just like how the message descriptor is a sequence of field descriptors, the
encoding type of a message is an ordered list of encoding fields.
\\
\begin{definition}[Field Compatibility]
	For fields $f_1 = (id_1, tag_1, v_1)$ and $f_2 = (id_2, tag_2, v_2)$, $f_2$
	with descriptor $d_2$ is a compatible update to $f_1$ with descriptor $d_1$ if
	$id_1 = id_2$ and for all values $v_1$, $serialize_{d_1}\ f_1 = bs$ and there
	exists a $v_2$ such that $parse_{d_2}\ bs = v_2$ and $v_1 \prec v_2$ according to
	some relation $\prec$.
\end{definition}

An example value relation is given in Section~\ref{sec:val-rel}.
\\
\begin{definition}[Message Compatibility]
	A protobuf message descriptor $d_2$ is a compatible update to $d_1$ if every
	field in $d_1$ has a compatible field in $d_2$.
\end{definition}

Formally speaking, the message compatibility relation ($\preceq$) is given in
Section~\ref{sec:comp-rel}.
\\
\begin{definition}[Tag Function]
  The tag function $tag : \text{Type} \rightarrow \text{Tag}$ maps the \texttt{proto}
  level type to the corresponding tag as listed in Table~\ref{tab:tags}.
\end{definition}

It is also important to understand the default values of each protobuf
type. When a field is serialized with the default value or not set in the
message, the parser places the default value in the resulting struct. The
primary difference between an implicit field and an optional field is that the
optional field can differentiate between the serialized field being omitted from
the message or being present, but having the default value.
\\
\begin{definition}[Default Function]
  The default function
  $default: (t:\text{proto type}) \rightarrow \llbracket t \rrbracket$ returns the default
  value for each type.
  
  \[ default(t) = \left\{ 
    \begin{array}{l@{\quad:\quad}l}
      0 & t \in \left\{\begin{array}{c}
        \mathtt{int32}, \mathtt{int64}, \mathtt{uint32},
        \mathtt{uint64}, \mathtt{sint32}, \mathtt{sint64}, \\ \mathtt{fixed32},
        \mathtt{fixed64}, \mathtt{sfixed32}, \mathtt{sfixed64}, \mathtt{enum}
      \end{array}\right\} \\
      \mathtt{false} & t \text{ is } \mathtt{bool} \\
      ``" & t \text{ is } \mathtt{string} \\
      % 0 width space needed so that the brackets don't parse as a distance to
      % space out the elements of the array... for some unknown reason.
      \hspace{0pt}[\;] & t \text{ is } \mathtt{bytes}
    \end{array}\right.
  \]
\end{definition}


\section{Compatibility Relations}~\label{sec:comp-rel}

Compatibility between protobuf entities is defined with a series of
relations. The first and most self-contained relation is the value relation,
which relates two values at the specification level if serializing just a value
(such as a variable width integer) and then parsing it to a different protobuf
type would lead to a different value. After that is the protobuf type relation,
which relates two protobuf type is any value of the first type is related to
some value of the second type in the value relation. Since protobuf field and
message definitions will not have concrete values associated with them, the type
relation can be used in place of the value relation to make safe type
changes. Finally, the message relation connects compatible messages such that
all instances of the original message can be parsed into instances of the updated
message.

\subsection{Value Relation}~\label{sec:val-rel}

The value relation is the base level of the compatibility relations, relating
just two individual values. Protobuf values are modeled here as a specification
level type in \fstar{} associated with a protobuf type as listed in
Table~\ref{tab:val-spec}. Finally, while protobuf decorators are technically
part of the field level specification, they have been incorporated into the type
information to allow the specification type into include \texttt{option} or
\texttt{list} \fstar{} types.

\begin{table}[H]
	\centering
	\begin{tabular}{cl}
      \toprule
      Specification Type & Protobuf Type \\
      \midrule
      $\mathbb{Z}$ & \texttt{int32}, \texttt{uint32}, \texttt{sint32}
            \texttt{int64}, \texttt{uint64}, \texttt{sint64} \\
      $\mathbb{B}$ & \texttt{bool}    \\
      \texttt{string} & \texttt{string}    \\
      \texttt{list char}  & \texttt{bytes}    \\
      -- & \texttt{float}, \texttt{double} \\
      \bottomrule
	\end{tabular}

	\vspace{4mm}
	\caption[]{Description of the \fstar{} type corresponding to each protobuf
      type. Since \fstar{} doesn't support any type of floating point number,
      specification types cannot be provided for floating point protobuf types.}\label{tab:val-spec}
\end{table}

\begin{definition}[Value Relation]
  A protobuf value at the specification level $v_1$ with protobuf type $\tau_1$ is
  related to another value $v_2$ at type $\tau_2$, denoted $v_1 : \tau_1 \prec v_2 : \tau_2$
  if $parse_{\tau_2}\ serialize_{\tau_1}\ v_1 = v_2$.
\end{definition}

\subsubsection{Basic Rules}

The value relation is reflexive and transitive.

\begin{mathpar}
  \infer[Refl]{ }{v:\tau \prec v:\tau}

  \infer[Trans]{v_1:\tau_1 \prec v_2:\tau_2 \\ v_2:\tau_2 \prec v_3:\tau_3}{v_1:\tau_1 \prec v_3:\tau_3}
\end{mathpar}

\subsubsection{String \& Byte Rules}

It is possible to convert been the \texttt{string} and \texttt{bytes} types,
although the host program may have to deal with control character bytes being
present in the resulting string.

\begin{mathpar}
  \infer[Str-Byte]{ }{v : \mathtt{string} \prec v : \mathtt{bytes}}

  \infer[Byte-Str]{ }{v : \mathtt{bytes} \prec v : \mathtt{string}}
\end{mathpar}

\subsubsection{Integer Rules}

As a shorthand, statements like \texttt{uint[n]} represent a variable length
integer encoding into $n$ bits. In order to result in valid protobuf types, $n \in
\{32, 64\}$. The change width rules are designed to allow for both integer
promotion and demotion while the rest of the rules express what happens when
converting between integers using different encoding types for negative numbers.

With regard to the \textsc{Int-Chg-W} rule, I was originally concerned about
negative 32 bit numbers being encoded into 5 bytes for a varint encoding and
then becoming positive if parsed into 64 bit integer, but protobuf handles this
by writing all negative \texttt{int[n]} into a full 10 bytes. 

It is also worth noting that the expression $v : \tau$ actually refers to a value
of type $\llbracket \tau \rrbracket$ with a label of the type. All of the integer
types have $\llbracket \cdot \rrbracket = \mathbb{Z}$, as shown in Table~\ref{tab:val-spec},
which allows for the free conversion between integer types. Finally, $\%$ refers
to modulus using floored division, so $-1 \mod 2 = 1$ and the rules are
written with truncated integer division.

\begin{mathpar}
  \infer[Uint-Chg-W]{v_2 = v_1 \mod 2^m}{v_1 : \mathtt{uint[n]} \prec v_2 : \mathtt{uint[m]}}

  \infer[Int-Chg-W]{v_2 = (v_1 \mod 2^{m-1} - 2^{m-1} \times \mathbb{1}[v_1 < 0]}{v_1
    : \mathtt{int[n]} \prec v_2 : \mathtt{int[m]}}

  \infer[Sint-Chg-W]{v_2 = v_1 \mod 2^{m-1} - 2^{m-1} \times
    \mathbb{1}\left[\frac{v_1}{2^{m-1}} \mod 2 = 1\right]}{v_1 :
    \mathtt{sint[n]} \prec v_2 : \mathtt{sint[m]}}

  \infer[Uint-Int]{v_2 = v_1 - 2^n \times \mathbb{1}[v_1 \ge 2^{n-1}]}{v_1 :
    \mathtt{uint[n]} \prec v_2 : \mathtt{int[n]}}

  \infer[Int-Uint]{v_2 = v_1 + 2^n \times \mathbb{1}[v_1 < 0]}{v_1 :
    \mathtt{int[n]} \prec v_2 : \mathtt{uint[n]}} 

  \infer[Uint-Sint]{v_2 = (-1)^{v_1} \times \left( \frac{v_1}{2} \right) - (v_1 \%
    2)}{v_1 : \mathtt{uint[n]} \prec v_2 : \mathtt{sint[n]}} 

  \infer[Sint-Uint]{v_2 = 2 \times |v_1| - \mathbb{1}[v_1 < 0]}{v_1 :
    \mathtt{sint[n]} \prec v_2 : \mathtt{uint[n]}} 

  \infer[Int-Sint]
  {
    v_2 = \left\{
      {\begin{array}{l@{\quad:\quad}l}
        (-1)^{v_1} \times \left( \frac{v_1}{2} \right) - (v_1 \mod 2) & \text{if } v_1 \ge 0 \\
        (-1)^{v_1} \times \left(v_1 + 2^{n-1} - \frac{v_1}{2} \right) & \text{otherwise}
      \end{array}}\right.
  }
  {v_1 : \mathtt{int[n]} \prec v_2 : \mathtt{sint[n]}}

  \infer[Sint-Int]
  {
    v_2 = \left\{
      {\begin{array}{l@{\quad:\quad}l}
        2 \times |v_1| - \mathbb{1}[v_1 < 0] & \text{if } -2^{n-2} \le v_1 < 2^{n-2} \\
        2 \times |v_1| - 2^n - \mathbb{1}[v_1 < 0] & \text{otherwise}
      \end{array}}\right.
  }
  {v_1 : \mathtt{sint[n]} \prec v_2 : \mathtt{int[n]}}
\end{mathpar}

\subsubsection{Boolean Rules}

Booleans can be converted to and from any of the integer types.

\begin{mathpar}
  \infer[Uint-Bool]
  {
    v_2 = \left\{
      {\begin{array}{l@{\quad:\quad}l}
        \mathtt{false} & \text{if } v_1 = 0 \\
        \mathtt{true} & \text{otherwise}
      \end{array}}\right.
  }
  {v_1 : \mathtt{uint[n]} \prec v_2 : \mathtt{bool}}

  \infer[Bool-Uint]{v_2 = \mathbb{1}[v_1]}{v_1 : \mathtt{bool} \prec v_2 :
    \mathtt{uint[n]}} 

  \infer[Int-Bool]
  {
    v_2 = \left\{
      {\begin{array}{l@{\quad:\quad}l}
        \mathtt{false} & \text{if } v_1 = 0 \\
        \mathtt{true} & \text{otherwise}
      \end{array}}\right.
  }
  {v_1 : \mathtt{int[n]} \prec v_2 : \mathtt{bool}}

  \infer[Bool-Int]{v_2 = \mathbb{1}[v_1]}{v_1 : \mathtt{bool} \prec v_2 :
    \mathtt{int[n]}} 

  \infer[Sint-Bool]
  {
    v_2 = \left\{
      {\begin{array}{l@{\quad:\quad}l}
        \mathtt{false} & \text{if } v_1 = 0 \\
        \mathtt{true} & \text{otherwise}
      \end{array}}\right.
  }
  {v_1 : \mathtt{sint[n]} \prec v_2 : \mathtt{bool}}

  \infer[Bool-Sint]{v_2 = -\mathbb{1}[v_1]}{v_1 : \mathtt{bool} \prec v_2 :
    \mathtt{sint[n]}} 
\end{mathpar}

\subsubsection{Message \& Enum Rules}

Definitions for the structure of a message and enum are given in
Section~\ref{sec:msg-rel}. The values are represented as a mapping from the id
number of each field to the value and type of that field. 

\begin{mathpar}
  \infer[Msg]{\forall\ i \in ids(v_{m_2}).\ (i \in ids(v_{m_1}) \wedge v_i \prec v_i') \vee v_i' =
    default(\tau_i')}{v_{m_1} = \left\{ id_1:(v_1, \tau_1) \dots, id_n: (v_n, \tau_n)
    \right\} : \mathtt{MSG}\ m_1 \prec v_{m_2} = \left\{ id_1':(v_1', \tau_1') \dots,
      id_m':(v_m', \tau_m') \right\} : \mathtt{MSG}\ m_2}

  \infer[Enum]{v \in vals(e_2)}{v : \enum{s_1}{e_1} \prec v : \enum{s_2}{e_2}}
\end{mathpar}

\subsubsection{Decorator Rules}

Since the specification type can include \texttt{option}'s and \texttt{list}'s,
rules for handling these must also be present in the value relation. These rules
are grouped into introduction rules, which take implicit values and make them
optional or repeated, pass through rules which enable the above rules to operate
on these modified types.

{\color{red} There really should be a rule which can introduce a \texttt{None},
  but since implicit fields are modeled as just the spec type, this is
  difficult. I will likely need to model both implicit and optional fields with
  an \texttt{option}, add an annotation to the protobuf type and provide a
  function which introduces a default value for each type.}

\begin{mathpar}
  \infer[Rep-Pass]{v_1:\tau_1 \prec v_1':\tau_2 \\ \cdots \\ v_n:\tau_1 \prec v_n':\tau_2}{[v_1; \dots;
    v_n]: \rep{\tau_1} \prec [v_1'; \dots; v_n']: \rep{\tau_2}}

  \infer[Opt-Some-Pass]{v_1:\tau_1 \prec v_2:\tau_2}{(\some{v_1}): \opt{\tau_1} \prec
    (\some{v_2}): \opt{\tau_2}}

  \infer[Opt-None-Pass]{ }{\none: \opt{\tau_1} \prec \none: \opt{\tau_2}}
  
  \infer[Opt-Intro]{ }{v:\tau \prec (\some{v}): \opt{\tau}}

  \infer[Rep-Intro]{ }{v:\tau \prec [v]: \rep{\tau}}

  \infer[Missing-Imp]{ }{\none: \tau_1 \prec default(\tau_2):\tau_2}
\end{mathpar}

\subsection{Type Relation}~\label{sec:typ-rel}

This relation relates types which can be converted via the value relation.
\\
\begin{definition}[Type Relation]
  The type relation relates two protobuf type $\tau_1$ and $\tau_2$, denoted $\tau_1 \propto
  \tau_2$ if for all $v_1:\tau_1$ there exists a $v_2:\tau_2$ such that $v_1:\tau_1 \prec v_2:\tau_2$.
\end{definition}

\subsubsection{Base Type Rules}

\begin{mathpar}
  \infer[Str-Byt-T]{ }{\mathtt{string} \propto \mathtt{bytes}}

  \infer[Byt-Str-T]{ }{\mathtt{bytes} \propto \mathtt{string}}

  \infer[Int-Int-T]{\tau_1, \tau_2 \in \left\{{
        \begin{array}{c}
          \texttt{int32}, \texttt{int64}, \texttt{uint32}, \texttt{uint64} \\
          \texttt{sint32}, \texttt{sint64}, \texttt{bool}
        \end{array}
      }\right\}}{\tau_1 \propto \tau_2}
\end{mathpar}

\subsubsection{Message \& Enum Rules}

\begin{mathpar}
  \infer[Msg-T]{m_1 \preceq m_2}{\mathtt{MSG}\ m_1 \propto \mathtt{MSG}\ m_2}

  \infer[Enum-T]{vals(e_1) \subset vals(e_2)}{\enum{s_1}{e_1} \propto \enum{s_2}{e_2}}
\end{mathpar}

\subsubsection{Decorator Rules}

Since the type relation is designed for checking for safe type changes, it also
need to ensure that decorator restrictions like moving from a repeated field to
a singleton one aren't violated.

\begin{mathpar}
  \infer[Opt-Add-T]{ }{\tau \propto \opt{\tau}}

  \infer[Opt-Rm-T]{ }{\opt{\tau} \propto \tau}

  \infer[Opt-T]{\tau_1 \propto \tau_2}{\opt{\tau_1} \propto \opt{\tau_2}}

  \infer[Rep-Add-T]{ }{\tau \propto \rep{\tau}}

  \infer[Rep-T]{\tau_1 \propto \tau_2}{\rep{\tau_1} \propto \rep{\tau_2}}
\end{mathpar}

\subsection{Message Relation}~\label{sec:msg-rel}

The descriptor compatibility relation $\preceq$ is defined as below, but first it is
important to discuss the structure of fields and messages.

Each field is one of three different type of fields, the standard scalar field,
a map field or a \texttt{oneof} field. Specifically, the inductive type for
each style of field contains these pieces of information:

\begin{itemize}
\item \texttt{FIELD}: Contains the name of the field, the identifying field
  number and protobuf type (containing the decorator as well, possibly
  \texttt{OPT} or \texttt{REP}).
\item \texttt{MAP}: Contains the name of the field, the identifying field number
  and two protobuf types, one for the keys and one of the values.
\item \texttt{ONEOF}: Contains a name and a set of other fields.
\end{itemize}

A message descriptor and an enum descriptor are modeled one step above a field,
containing the following pieces of information:

\begin{itemize}
\item \texttt{MSG}: Contains the name of the message, a set of reserved indices
  and then a set of field definitions.
\item \texttt{ENUM}: Contains a set of name, id number pairs.
\end{itemize}

\subsubsection{Basic Rules}

A compatibility relation is both reflexive and transitive, although it is
neither symmetric nor anti-symmetric.  

\begin{mathpar}
   \infer[Refl-M]{ }{\msg{s}{r}{f} \preceq \msg{s}{r}{f}}

   \infer[Refl-E]{ }{\enum{s}{v} \preceq \enum{s}{v}}

   \infer[Trans-M]{\msg{s_1}{r_1}{f_1} \preceq \msg{s_2}{r_2}{f_2} \\
     \msg{s_2}{r_2}{f_2} \preceq \msg{s_3}{r_3}{f_3}}{\msg{s_1}{r_1}{f_1} \preceq
     \msg{s_3}{r_3}{f_3}}

   \infer[Trans-E]{\enum{s_1}{v_1} \preceq \enum{s_2}{v_2} \\
     \enum{s_2}{v_2} \preceq \enum{s_3}{v_3}}{\enum{s_1}{v_1} \preceq \enum{s_3}{v_3}}
\end{mathpar}

\subsubsection{Field Update Rules}

For the moment, names are recorded the protobuf field descriptors for accuracy
and to allow the relations to expand in the future. However, since names are
recorded in the encoded message, they are currently allowed to freely change.

\begin{mathpar}
  \infer[Field-Name]{ }{\msg{s_m}{r}{(f \cup \{\field{s_f}{id}{\tau}\})} \preceq
    \msg{s_m}{r}{(f \cup \{\field{s_f'}{id}{\tau}\})}}

  \infer[Map-Name]{ }{\msg{s_m}{r}{(f \cup \{\map{s_f}{id}{\tau_1}{\tau_2}\})} \preceq
    \msg{s_m}{r}{(f \cup \{\map{s_f'}{id}{\tau_1}{\tau_2}\})}}

  \infer[Oneof-Name]{ }{\msg{s_m}{r}{(f \cup \{\oneof{s_f}{f_o}\})} \preceq
    \msg{s_m}{r}{(f \cup \{\oneof{s_f'}{f_o}\})}}

  \infer[Field-Type]{\tau_1 \propto \tau_2}{\msg{s_m}{r}{(f \cup \{\field{s_f}{id}{\tau_1}\})} \preceq
    \msg{s_m}{r}{(f \cup \{\field{s_f}{id}{\tau_2}\})}}

  % TODO: May need restrictions about maps in maps...
  \infer[Map-Type-F]{\tau_1 \propto \tau_1' \\ \tau_2 \propto \tau_2' \\ \tau_1 \ne
    \mathtt{bytes}}{\msg{s_m}{r}{(f \cup \{\map{s_f}{id}{\tau_1}{\imp{\tau_2}}\})} \preceq
    \msg{s_m}{r}{(f \cup \{\map{s_f}{id}{\tau_1'}{\imp{\tau_2'}}\})}}
\end{mathpar}

\subsubsection{Oneof Rules}

The \texttt{oneof} rules are a bit complicated because a \texttt{oneof}
struggles against the notion that the field relations only connects one field to
one field by encapsulating several fields into a singular field.

\begin{mathpar}
  \infer[Oneof-Intro-Field]{m \in \{\mathtt{IMP}, \mathtt{OPT}\}}{\msg{s_m}{r}{(f
      \cup \{\field{s_f}{id}{(m\ \tau)}\})} \preceq \msg{s_m}{r}{(f \cup
      \{\oneof{s_o}{\{\field{s_f}{id}{(m\ \tau)}\}}\})}}

  % May need some statement about f_{n+1} not already being in the message?
  % You can introduce a new field, but not move an existing one.
  \infer[Oneof-Add-F]{m \in \{\mathtt{IMP}, \mathtt{OPT}\} \\ s_n \not \in names(f \cup
    f_o) \\ id_n \not \in ids(f \cup f_o)}{\msg{s_m}{r}{f \cup \{\oneof{s_f}{f_o}\}} \preceq
    \msg{s_m}{r}{f \cup \{\oneof{s_f}{(f_o \cup \{\field{s_n}{id_n}{(m\ \tau)}\})}\}}}

  % This rule should just be a combination of the above two rules...
  \infer[Oneof-Intro-New]{s_f \not \in names(f) \\ ids(f) \cap ids(f_o) = \emptyset \\
    names(f) \cap names(f_o) = \emptyset \\\\ \mathtt{REP} \not \in dec(f_o) \\ \forall f_n \in f_o.\
    f \ne \mathtt{MAP}}{\msg{s_m}{r}{f} \preceq \msg{s_m}{r}{(f \cup
      \{\oneof{s_f}{f_o}\})}}

  \infer[Oneof-Elim]{ }{\msg{s_m}{r}{(f \cup \{\oneof{s_f}{f_o}\})} \preceq \msg{s_m}{r}{(f
      \cup \{f_o\})}}

  % I'm not sure about the premise of this rule, but it seems better than having
  % to making another version of all the other rules. This is the first instance
  % where having a field relation would be very helpful.
  \infer[Oneof-Feild-Update]{\msg{``"}{\emptyset}{\{f\}} \preceq
    \msg{``"}{\emptyset}{\{f'\}}}{\msg{s}{r}{(f_m \cup \{\oneof{s_f}{(f_{o} \cup \{f\})}\})} \preceq
    \msg{s}{r}{(f_m \cup \{\oneof{s_f}{(f_o \cup \{f'\})}\})}}
\end{mathpar}

\subsubsection{Field \& Enum Rules}

Messages are similar to records in \fstar{} or other functional languages and
message compatibility can take the same two routes that record subtyping can:
width compatibility and depth compatibility. In width compatibility updates, new
fields are added to the message, making the updated version a strict superset of
the original message. In depth compatibility updates, the number of fields
remains the same, but other attributes of the field are updated compatible ways.

\begin{mathpar}
  \infer[Msg-Width-Field]{s_f \not \in names(f) \\ id \not \in
    ids(f)}{\msg{s_m}{r}{f} \preceq \msg{s_m}{r}{(f \cup \{\field{s_f}{id}{\tau}\})}}

  \infer[Msg-Width-Depth]{\msg{``"}{\emptyset}{\{f\}} \preceq
    \msg{``"}{\emptyset}{\{f'\}}}{\msg{s_m}{r}{(fs \cup \{f\})} \preceq
    \msg{s_m}{r}{(fs \cup \{f'\})}}
\end{mathpar}

The enum rules are similar to the message rules at least in the sense that enums
can be updated to have new fields without issue. However, when considering
removing fields from an enum definition, it is important to know if the enum is
treated as open or closed~\cite{EnumBehavior}. By default, \texttt{proto3}
treats all enums as open, so it should be possible to write a rule allowing for
the deletion of an enum field. This naturally aligns with how \texttt{go}
handles enums, since it doesn't support closed enums. Since protobuf enums don't
contain extra information, no depth rules is allowed.

\begin{mathpar}
  \infer[Enum-Width]{vals(e_1) \subset vals(e_2)}{\enum{s_1}{e_1} \preceq \enum{s_2}{e_2}}
\end{mathpar}

\subsubsection{Reserved Field Rules}

\begin{mathpar}
  \infer[Reserved-Add]{r \subset r'}{\msg{s}{r}{f} \preceq \msg{s}{r'}{f}}

  {\color{red}
    \infer[Reserved-Rm]{r \supset r'}{\msg{s}{r}{f} \preceq \msg{s}{r'}{f}}
  }
\end{mathpar}

\subsubsection{Valid Descriptors}

\begin{definition}[Valid Protobuf Descriptors]
  A protobuf message descriptor is \emph{valid} if $\msg{``"}{\emptyset}{\emptyset} \preceq \msg{s}{r}{f}$.
\end{definition}

\subsubsection{Compatibility Theorem}

The main field-level compatibility theorem is stated below.
\\
\begin{theorem}[Field Compatibility]
  If $d_1 \preceq d_2$ then for all binary strings $bs$ such that
  $\mathtt{parse}_{d_1}\ bs = \mathtt{Some}\ v_1$, $\mathtt{parse}_{d_2}\ bs =
  \mathtt{Some}\ v_2$ and $v_1 \prec v_2$
\end{theorem}

\printbibliography{}
\end{document}

% Local Variables:
% citar-bibliography: ("./pollux.bib")
% TeX-command-default: "Make"
% jinx-local-words: "Everparse proto protobuf protoc"
% End:
