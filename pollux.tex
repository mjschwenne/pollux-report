\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{nicematrix}
\usepackage{mathpartir}
\usepackage{parskip}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usetikzlibrary{positioning,calc,snakes,automata,arrows.meta}
\usepackage{array}
\usepackage{multirow}
\usepackage{listings}
\usepackage{stmaryrd}
\usepackage[backend=biber,sorting=ynt]{biblatex}
\addbibresource{pollux.bib}
\author{Matt Schwennesen}
\title{Pollux: Protobuf Compatibility Checking}
\date{}
\hypersetup{
 pdfauthor={Matt Schwennesen},
 pdftitle={Pollux},
 pdfkeywords={},
 pdfsubject={},
 pdflang={English}
}

\lstdefinelanguage{proto}{
  keywords={message,double,float,int32,int64,uint32,uint64,sint32,sint64,fixed32,fixed64,sfixed32,sfixed64,bool,string,bytes,oneof,enum}
}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{pollux}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=pollux}

\newcommand{\fstar}{F\(^\star\)}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}

Software has become an integral and integrated part of everyday life, and with
that comes a near constant stream of updates to various devices all around
us. Unfortunately, software updates are a frequent source of issues
\autocites{zhangUnderstandingDetectingSoftware2021}[][]{Gray1986WhyDC}, often
making it into production environments before the necessary edge cases are
triggered. We believe that techniques from formal verification can be applied
here to ensure update compatibility, which is ultimately what my project aims to
do. Unlike some previous work
\autocites{ajmaniModularSoftwareUpgrades2006}[][]{reitblattAbstractionsNetworkUpdate2012},
I aim to prove compatibility of software updates without imposing restrictions
about how the update itself is performed.

The most nebulous part of this goal is how to define ``compatible'', which I
break down into two major categories; data compatibility and operational
compatibility. Data compatibility aims to show that that the new version can
correctly interpret persistent state left behind by the pervious version while
operational compatibility reasons about behavior of invoking the same function
or feature in both versions of the software. This project is currently working
on data compatibility.

Imagine that you're a developer for a distributed system using \texttt{protobuf}
and \texttt{grpc} to communicate between nodes. A new version of
\texttt{protobuf} just came out and added a syntax for maps to the
protocol. When you used to need maps, you had to manually encode a list of
key-value pairs but now you can write \texttt{map<string, int32>} in the
\texttt{.proto} schema file. Is changing between these syntactic definitions a
compatible update? From a high level, they both model the same mathematical map,
which is the information the program is actually trying to communicate, so it
should be possible to define a compatible update this way. The specific answer
naturally depends on how the new \texttt{map} is encoded into binary, and in the
case of \texttt{protobuf} a \texttt{map} is syntactic sugar for a list of
key-value pairs so this is a valid update\footnote{There is slightly more to it
	than that, regarding some \texttt{optional} tags which might not present in
	the original definition but it is possible to define this update in a
	compatible way}. While this example is considering a rolling update to a
distributed system, the same question would apply to an application which writes
the serialized blob to disk and reads it when the application starts up again.

The notion of both types of maps encoding the same mathematical map leads us to
the idea of abstract state. When we're programming, we don't often think about
which bucket an element of a hashmap is in but rather we think at a higher
level. It is possible to conceptualize this formally, were we have some abstract
state and a concrete state which are linked with a predicate asserting that the
abstract state is captured by the concrete state. This is the same relationship
we see in proofs of functional correctness, were the specification of a function
corresponds to the abstract state and the implementation to the concrete
state. This relationship will serve as the theoretical backing to the project.

More practically, consider an RPC or other serialization library such as
Protobuf or Cap'n'Proto. Updates to RPC schema are an excellent place to start
exploring notions of data compatibility since RPC schema are separated from the
source code that operates on them, are highly structured serialized formats and
are designed to allow the schema to evolve though updates. Tools already exist
which can report on if two versions of a schema file are compatible, these tools
operate using ad-hoc notions of compatibility asserting a specification or
verifying the implementation. Using \fstar, I plan to verify a compatibility
checker for one of these libraries before expanding the project to consider a
wider range of data formats. It should also be possible to include some analysis
of the source code operating on the state to prove more invasive updates, or
even the proofs of verified pieces of software, although that is a goal for a
longer timescale.

\section{Abstract State Schema}

In software verification, an implementation is always verified against some
specification of correct behavior. This is implicitly part of
\texttt{protobuf} too; when a message is serialized and transmitted over the
network, the developer expects that the same structure can be recreated by the
receiving program. Refactoring this intuition to be more directly applicable to
the standard verification setup, one could argue that the binary wire blob
serves as an ``implementation'' of the structure or object in the host
language (be it \texttt{go}, \texttt{java}, \texttt{python} or any other
language with \texttt{protobuf} bindings) which itself is generated by
\texttt{protoc} and is an ``implementation'' of the proto schema defined in a
\texttt{.proto} file. This stack defines the \texttt{.proto} file as the top
level source of truth, but this can be modeled by the high level host language
in most cases. From the prospective of an application developer, the host
language representation is the most important, since this is where the schema
structure interacts with other parts of the host application.

The definition of a \texttt{proto} message can be modeled by an \fstar{} type,
in a way similar to structures created by \texttt{protoc} for other host
languages. This is not fully sufficient though, since the representation
exposed by \texttt{protoc} isn't enough to fully reason about compatibility.

\subsection{\fstar{} Proto Struct Representation}

Some parts of the translation from a \texttt{proto} message
\autocite{LanguageGuideProto} into \fstar{} is simple. For example, a 64 bit
unsigned integer can be represented with a \texttt{UInt64} from the \fstar{}
machine integer library. Likewise a message can be represented by a record
\autocite{swamy2023proof}. Not everything cleanly generalizes
though. Specifically, it is unclear to how represent some primitive types such
as \texttt{float}, \texttt{double} or \texttt{string}.

\begin{table}[H]
	\centering
	\begin{tabular}{ll}
		\toprule
		\texttt{proto} Feature & \fstar{} Representation   \\
		\midrule
		Integers               & Machine Integer Library   \\
		\texttt{string}        & \texttt{string}           \\
		\texttt{message}       & Record                    \\
		\texttt{optional}      & \texttt{option}           \\
		\texttt{repeated}      & \texttt{list}             \\
		\texttt{map}           & ???                       \\
		\texttt{bool}          & \texttt{bool}             \\
		\texttt{bytes}         & \texttt{Seq.seq UInt8.t}  \\
		\texttt{enum}          & Inductive type            \\
		\texttt{oneof}         & Inductive type (Sum type) \\
		\texttt{option}        & ???                       \\
		\bottomrule
	\end{tabular}

	\vspace{4mm}
	\caption[]{\texttt{proto} language features and corresponding \fstar{} representations.}
\end{table}

Notice that there are several unknown representations in the table. Below is
some commentary on these features:

\begin{itemize}
	\item \texttt{map}: As far as I can tell, \fstar{} doesn't have maps, however
	      this \fstar{} development doesn't \emph{need} maps beyond a way to serialize
	      and de-serialize them. We should be able to develop a parametric
	      \texttt{map} definition as a list of tuples, just like how protobuf would
	      serialize them.
	\item \texttt{option}: This isn't an optional value, but rather top level
	      options used to set things like the \texttt{java} package. At the moment,
	      I'm not sure if or how changing values can impact protobuf compatibility.
\end{itemize}

A good completeness test might be encoding \texttt{descriptor.proto}, the proto
file which can encode other protobuf schema.

While this would provide a framework for building \fstar{} bindings for
protobuf, a richer representation for a protobuf schema for checking
compatibility at the wire level.

\subsection{Compatibility Checking with \fstar{} Types}

When a protobuf message is encoding, field names are stripped out in favor of
field numbers or field tags, which are manually defined in the protobuf
schema. Consider the two \texttt{proto} snippets below:

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    int32 bar = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{1cm}
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    int32 bar = 2;
}\end{lstlisting}
	\end{minipage}

	\caption[]{Example showing the importance of including proto field numbers in
		compatibility checking.}
\end{figure}

Any invocation of \texttt{protoc} will generate exactly the same
application-facing code, even though a message generated using the left proto
file will be parsed to an empty message by the right proto file.

This suggests that a more descriptive type is required. Rather than model the
struct that would be output by \texttt{protoc}, the source type for Pollux
should target the descriptor, as represented in the \texttt{.proto} file. Then,
in a manner similar to the \texttt{parse} function in Everparse, develop
something like this:

\begin{align*}
	\mathtt{Desc}               & : Type                                                                             \\
	\mathtt{Record}             & : Type \quad \text{Record corresponding to descriptor}                             \\
	\llbracket \cdot \rrbracket & : \mathtt{Desc} \rightarrow \mathtt{Record}                                        \\
	parse                       & : (d:\mathtt{Desc}) \rightarrow \mathtt{Bytes} \rightarrow option\
	\llbracket d \rrbracket                                                                                          \\
	serialize                   & : (d:\mathtt{Desc}) \rightarrow \llbracket d \rrbracket \rightarrow \mathtt{Bytes}
\end{align*}

Several shorthands arise from this, namely $parse_d : \mathtt{Bytes} \rightarrow option\
	\llbracket d \rrbracket$ for the partial application of a descriptor $d$ to
$parse$. The same thing can be done for $serialize_d : \llbracket d \rrbracket \rightarrow
	\mathtt{Bytes}$, which naturally to more traditional definitions of a parser and serializer.

\section{Protobuf Compatibility Questions}

As the \fstar{} development moves forward, it is beneficial to have a minimal
working example of a non-trivial compatibility question for protocol
buffers. There are several clearly trivial questions, like updating a
\texttt{int32} to an \texttt{int64} using the variable width integer encoding.

The protobuf encoding format is simple, which makes it challenging to construct
non-trivial compatibility question from small, trivial protobuf descriptions. As
an example, consider this example:

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Bar {
    string baz = 1;
}

message Foo {
    Bar bar = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{1cm}
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    string bar = 1;
}\end{lstlisting}
	\end{minipage}

	\caption[]{A compatibility question.}
\end{figure}

The data is correctly exposed in the string on the right but there will be
several junk bytes at the beginning that the application has to trim. As a
definitional question, should this be considered compatible? Where or not a
client application can detect and correct the contents of the \texttt{bar} field
is beyond the current scope of this project.

From an encoding prospective, a protobuf message is encoded (mostly) as a
sequence of id, tag and value tuples. The id is the field number encoded in the
proto descriptor, the tag marks the type of value to follow and finally the
value contains the content set by the encoding application. More specifically,
the tag is from the table below.

\begin{table}[H]
	\centering
	\begin{tabular}{cll}
		\toprule
		ID & Name            & Uses                                                 \\
		\midrule
		0  & \texttt{VARINT} & \texttt{int32}, \texttt{int64}, \texttt{uint32},
		\texttt{uint64}, \texttt{sint32}, \texttt{sint64},
		\texttt{bool}, \texttt{enum}                                                \\
		1  & \texttt{I64}    & \texttt{fixed64}, \texttt{sfixed64}, \texttt{double} \\
		2  & \texttt{LEN}    & \texttt{string}, \texttt{bytes}, embedded messages,
		packed repeated fields                                                      \\
		5  & \texttt{I32}    & \texttt{fixed32}, \texttt{sfixed32},
		\texttt{float}                                                              \\
		\bottomrule
	\end{tabular}

	\vspace{4mm}
	\caption[]{Description of \texttt{proto} tag values. Note that values 3 and
		4 correspond to \texttt{SGROUP} and \texttt{EGROUP}, tags used in protobuf
		version 2 and deprecated in protobuf 3 \autocite{Encoding}.}
\end{table}

It has finally become time for a sequence of definitions. A protobuf message
descriptor is comprised of a series of field descriptors, each of which has the
form \texttt{<<optional modifiers>> <<type>> <<field name>> = <<field
	number>>;}. During the encoding process, each field is encoded into an
encoding field, represented as tuple with 3 elements.

\begin{definition}
	An encoding field is a tuple $(id, tag, value)$ which encodes the field
	number, tag and value of one field of a proto descriptor.
\end{definition}

Just like how the message descriptor is a sequence of field descriptors, the
encoding type of a message is an ordered list of encoding fields.

\begin{definition}[Field Compatibility]
	For fields $f_1 = (id_1, tag_1, v_1)$ and $f_2 = (id_2, tag_2, v_2)$, $f_1$
	and $f_2$ are compatible with respect to a field descriptor $d_1$ and updated
	field descriptor $d_2$ if $id_1 = id_2$ and $serialize_{d_1}\ f_1 = bs$,
	$parse_{d_2}\ bs = v_2$ and $v_1 \sim v_2$ according to some relation $\sim$.
\end{definition}

One example of $\sim$ would be equality of mathematician integers represented with
different bit widths (i.e. 32-bit to 64-bit integers).

\begin{definition}[Message Compatibility]
	A protobuf message descriptor $d_2$ is a compatible update to $d_1$ if every
	field in $d_1$ has a compatible field in $d_2$.
\end{definition}

\printbibliography{}
\end{document}

% Local Variables:
% citar-bibliography: ("./pollux.bib")
% TeX-command-default: "Make"
% jinx-local-words: "Everparse proto protobuf protoc"
% End:
