\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{nicematrix}
\usepackage{mathpartir}
\usepackage{parskip}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usetikzlibrary{positioning,calc,snakes,automata,arrows.meta}
\usepackage{array}
\usepackage{multirow}
\usepackage{listings}
\usepackage{stmaryrd}
\usepackage[backend=biber,sorting=ynt]{biblatex}
\addbibresource{pollux.bib}
\author{Matt Schwennesen}
\title{Pollux: Protobuf Compatibility Checking}
\date{}
\hypersetup{
 pdfauthor={Matt Schwennesen},
 pdftitle={Pollux},
 pdfkeywords={},
 pdfsubject={},
 pdflang={English}
}

\lstdefinelanguage{proto}{
  keywords={message,double,float,int32,int64,uint32,uint64,sint32,sint64,fixed32,fixed64,sfixed32,sfixed64,bool,string,bytes,oneof,enum}
}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{pollux}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=pollux}

\newcommand{\fstar}{F\(^\star\)}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}

Software has become an integral and integrated part of everyday life, and with
that comes a near constant stream of updates to various devices all around
us. Unfortunately, software updates are a frequent source of issues
\autocites{zhangUnderstandingDetectingSoftware2021}[][]{Gray1986WhyDC}, often
making it into production environments before the necessary edge cases are
triggered. We believe that techniques from formal verification can be applied
here to ensure update compatibility, which is ultimately what my project aims to
do. Unlike some previous work
\autocites{ajmaniModularSoftwareUpgrades2006}[][]{reitblattAbstractionsNetworkUpdate2012},
I aim to prove compatibility of software updates without imposing restrictions
about how the update itself is performed.

The most nebulous part of this goal is how to define ``compatible'', which I
break down into two major categories; data compatibility and operational
compatibility. Data compatibility aims to show that that the new version can
correctly interpret persistent state left behind by the pervious version while
operational compatibility reasons about behavior of invoking the same function
or feature in both versions of the software. This project is currently working
on data compatibility.

Imagine that you're a developer for a distributed system using \texttt{protobuf}
and \texttt{grpc} to communicate between nodes. A new version of
\texttt{protobuf} just came out and added a syntax for maps to the
protocol. When you used to need maps, you had to manually encode a list of
key-value pairs but now you can write \texttt{map<string, int32>} in the
\texttt{.proto} schema file. Is changing between these syntactic definitions a
compatible update? From a high level, they both model the same mathematical map,
which is the information the program is actually trying to communicate, so it
should be possible to define a compatible update this way. The specific answer
naturally depends on how the new \texttt{map} is encoded into binary, and in the
case of \texttt{protobuf} a \texttt{map} is syntactic sugar for a list of
key-value pairs so this is a valid update\footnote{There is slightly more to it
	than that, regarding some \texttt{optional} tags which might not present in
	the original definition but it is possible to define this update in a
	compatible way}. While this example is considering a rolling update to a
distributed system, the same question would apply to an application which writes
the serialized blob to disk and reads it when the application starts up again.

The notion of both types of maps encoding the same mathematical map leads us to
the idea of abstract state. When we're programming, we don't often think about
which bucket an element of a hashmap is in but rather we think at a higher
level. It is possible to conceptualize this formally, were we have some abstract
state and a concrete state which are linked with a predicate asserting that the
abstract state is captured by the concrete state. This is the same relationship
we see in proofs of functional correctness, were the specification of a function
corresponds to the abstract state and the implementation to the concrete
state. This relationship will serve as the theoretical backing to the project.

More practically, consider an RPC or other serialization library such as
Protobuf or Cap'n'Proto. Updates to RPC schema are an excellent place to start
exploring notions of data compatibility since RPC schema are separated from the
source code that operates on them, are highly structured serialized formats and
are designed to allow the schema to evolve though updates. Tools already exist
which can report on if two versions of a schema file are compatible, these tools
operate using ad-hoc notions of compatibility asserting a specification or
verifying the implementation. Using \fstar{}, I plan to verify a compatibility
checker for one of these libraries before expanding the project to consider a
wider range of data formats. It should also be possible to include some analysis
of the source code operating on the state to prove more invasive updates, or
even the proofs of verified pieces of software, although that is a goal for a
longer timescale.

\section{Protobuf Primer}

Protobuf, or protocol buffers, is a message description language and
serialization protocol developed by Google. It is similar to other descriptive
data formats like ASN1, and is commonly used as part of the gRPC remote
procedure call system. Protobuf and gRPC are one of the most popular
serialization formats.

\subsection{The \texttt{proto} Language}

A high-level protobuf message description is written in a \texttt{.proto} file
in the \texttt{proto} language. Each message is described by a \emph{message
	descriptor} which declares each field and its corresponding type.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 results_per_page = 3;
}\end{lstlisting}

	\caption{Example protobuf message description from the protobuf documentation~\cite{LanguageGuideProto}.}
	\label{fig:proto-ex}
\end{figure}

In figure~\ref{fig:proto-ex}, a message called \texttt{SearchRequest} is
declared. This message contains three fields, the query, page number to display
and number of results per page. Each field is assigned a \emph{field number},
which is used in the serialized version of the message. Each field number must
be unique, as they are used to identify the corresponding binary blob in the
encoded message. A \texttt{.proto} file can define multiple messages, and
messages can be embedded in another by using the name of that message as the
type of a field in the other message.

Each field is defined with a type, with the primitive types summarized in
table~\ref{tbl:proto-prim}.

\begin{table}[H]
	\centering
	\begin{tabular}{cl}
		\toprule
		Type              & Notes                                                    \\
		\midrule
		\texttt{double}   & Standard 64 bit double-precision floating point
		number.                                                                      \\
		\texttt{float}    & Standard 32 bit floating point number.                   \\
		\texttt{int32}    & Variable-length signed 32 bit integer.                   \\
		\texttt{int64}    & Variable-length signed 64 bit integer.                   \\
		\texttt{uint32}   & Variable-length unsigned 32 bit integer.                 \\
		\texttt{uint64}   & Variable-length unsigned 64 bit integer.                 \\
		\texttt{sint32}   & Variable-length 32 bit integer with more
		efficient encoding for negative numbers.                                     \\
		\texttt{sint64}   & Variable-length 64 bit integer with more efficient
		encoding for negative numbers.                                               \\
		\texttt{fixed32}  & Four byte integer, more efficient than \texttt{uint32}
		for values greater than $2^{38}$.                                            \\
		\texttt{fixed64}  & Eight byte integer, more efficient than \texttt{uint64}
		for values greater than $2^{56}$.                                            \\
		\texttt{sfixed32} & Four byte signed integer.                                \\
		\texttt{sfixed64} & Eight byte signed integer.                               \\
		\texttt{bool}     & Boolean, encoded in one byte.                            \\
		\texttt{string}   & A string of UFT-8 or 7-bit ASCII characters shorter than
		$2^{32}$ bytes.                                                              \\
		\texttt{bytes}    & An arbitrary sequence of less than $2^{32}$ bytes.       \\
		\bottomrule
	\end{tabular}
	\caption{Summary of the protobuf scalar values.}
	\label{tbl:proto-prim}
\end{table}

Any protobuf field can marked as \texttt{optional} or \texttt{repeated}. An
\texttt{optional} field can be either set to a value or omitted from the
message. Similarly, a \texttt{repeated} field can be repeated in the message
zero or more times, functioning as a list now. A field without a cardinality
modifier is known as an implicit field and can also be omitted from a message,
just like an \texttt{optional} field. During de-serialization, a type-specific
default value will be used (this is consistent with accessing the value of an
unset \texttt{optional} field). The only difference between an implicit field
and \texttt{optional} field is that the optional field can differentiate between
an unset field and field set to the default value by providing a method which
reports if the \texttt{optional} field is set.

Since the protobuf language is designed to be both forward and backwards
compatible, it is possible to remove or add fields. When removing fields, it is
encouraged to reserve the field number to prevent reuse of the field in a
future. It is also possible to reserved field names, which isn't important for
the protobuf binary encoding format, this is important if you're using the JSON
encoding of a protobuf message.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

message Foo {
  reserved 2, 15, 9 to 11;
  reserved "foo", "bar";
}\end{lstlisting}

	\caption{Example protobuf message using reserved fields~\cite{LanguageGuideProto}.}
	\label{fig:proto-reserved}
\end{figure}

Protobuf also supports enumerations, for when a field may only take a finite
number of values. Due to how default values work in protobuf, enums are required
to have a value defined with a value zero and it should have either
``UNSPECIFIED'' or ``UNKNOWN'' since that will be the default value.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

enum {
  CORPUS_UNSPECIFIED = 0;
  CORPUS_UNIVERSAL = 1;
  CORPUS_WEB = 2;
  CORPUS_IMAGES = 3;
  CORPUS_LOCAL = 4;
  CORPUS_NEWS = 5;
  CORPUS_PRODUCTS = 6;
  CORPUS_VIDEO = 7;
}

message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 results_per_page = 3;
  Corpus corpus = 4;
}\end{lstlisting}

	\caption{Example protobuf message using an enum~\cite{LanguageGuideProto}.}
	\label{fig:proto-enum}
\end{figure}

There are two other features which need to be modeled, \texttt{map} and
\texttt{oneof}. Maps can be thought of like a standard map in \texttt{go} or a
\texttt{dict} in \texttt{python}. From a serialization perspective, this is an
unordered map that could be modeled as a list of key-value pairs. A \texttt{map}
field cannot be \texttt{repeated}, and repeated keys have the last occurrence
win, as is standard with protobuf (see Section~\ref{sec:proto-enc}).

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.43\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
  map<int32, string> map_field = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}[bt]{0.48\textwidth}
		\begin{lstlisting}[language=proto]
message MapFieldEntry {
  int32 key = 1;
  string value = 2;
}

message Foo {
  repeated MapFieldEntry map_field = 1;
}\end{lstlisting}
	\end{minipage}

	\caption{Example of \texttt{map} syntax and the corresponding
		wire-equivalent syntax~\cite{LanguageGuideProto}.}
	\label{fig:proto-map}
\end{figure}

The final important feature of the proto language is a \texttt{oneof} field,
which operates like a union in \texttt{C}. Setting any member of the
\texttt{oneof} will clear any previously set field. A \texttt{oneof} field
cannot contain a \texttt{repeated} field or a \texttt{map} and the
\texttt{oneof} field itself cannot be \texttt{repeated}.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

message User {
  oneof user_id {
    string email = 4;
    int32 phone = 2;
  }
}\end{lstlisting}

	\caption{Example protobuf message using an enum~\cite{LanguageGuideProto}.}
	\label{fig:proto-enum}
\end{figure}

\subsection{The Protobuf Encoding}\label{sec:proto-enc}

Complimenting the Protobuf schema language is the protobuf encoding format. All
valid protobuf messages written the the proto language will be eventually
encoding into a binary blob in respecting this encoding.

\subsubsection{Variable-Length Integers}

The heart of the protobuf encoding is the variable-length integer encoding,
which represents a 32 or 64 bit integer in between one and ten bytes. The
encoding format itself is relatively simple. The bits of the integer to be
encoded are grouped into bundles of seven. The bundles are ordered in little
endian order with the first bit of each byte set to one if the next byte is also
part of the same integer.

For example, consider the number 163, or 10100011 in binary. Splitting these
into bundles of seven bits produces ``0000001'' and ``0100011''. These are
reordered into little endian order, ``0000001'' then ``0100011''. Finally set
the continuation bit on the first byte of the encoding and concatenate them
together to get ``10000010100011''.

\section{Abstract State Schema}

In software verification, an implementation is always verified against some
specification of correct behavior. This is implicitly part of
\texttt{protobuf} too; when a message is serialized and transmitted over the
network, the developer expects that the same structure can be recreated by the
receiving program. Refactoring this intuition to be more directly applicable to
the standard verification setup, one could argue that the binary wire blob
serves as an ``implementation'' of the structure or object in the host
language (be it \texttt{go}, \texttt{java}, \texttt{python} or any other
language with \texttt{protobuf} bindings) which itself is generated by
\texttt{protoc} and is an ``implementation'' of the proto schema defined in a
\texttt{.proto} file. This stack defines the \texttt{.proto} file as the top
level source of truth, but this can be modeled by the high level host language
in most cases. From the prospective of an application developer, the host
language representation is the most important, since this is where the schema
structure interacts with other parts of the host application.

The definition of a \texttt{proto} message can be modeled by an \fstar{} type,
in a way similar to structures created by \texttt{protoc} for other host
languages. This is not fully sufficient though, since the representation
exposed by \texttt{protoc} isn't enough to fully reason about compatibility.

\subsection{\fstar{} Proto Struct Representation}

Some parts of the translation from a \texttt{proto} message
\autocite{LanguageGuideProto} into \fstar{} is simple. For example, a 64 bit
unsigned integer can be represented with a \texttt{UInt64} from the \fstar{}
machine integer library. Likewise a message can be represented by a record
\autocite{swamy2023proof}. Not everything cleanly generalizes
though. Specifically, it is unclear to how represent some primitive types such
as \texttt{float}, \texttt{double} or \texttt{string}.

\begin{table}[H]
	\centering
	\begin{tabular}{ll}
		\toprule
		\texttt{proto} Feature & \fstar{} Representation   \\
		\midrule
		Integers               & Machine Integer Library   \\
		\texttt{string}        & \texttt{string}           \\
		\texttt{message}       & Record                    \\
		\texttt{optional}      & \texttt{option}           \\
		\texttt{repeated}      & \texttt{list}             \\
		\texttt{map}           & ???                       \\
		\texttt{bool}          & \texttt{bool}             \\
		\texttt{bytes}         & \texttt{Seq.seq UInt8.t}  \\
		\texttt{enum}          & Inductive type            \\
		\texttt{oneof}         & Inductive type (Sum type) \\
		\texttt{option}        & ???                       \\
		\bottomrule
	\end{tabular}

	\vspace{4mm}
	\caption[]{\texttt{proto} language features and corresponding \fstar{} representations.}
\end{table}

Notice that there are several unknown representations in the table. Below is
some commentary on these features:

\begin{itemize}
	\item \texttt{map}: As far as I can tell, \fstar{} doesn't have maps, however
	      this \fstar{} development doesn't \emph{need} maps beyond a way to serialize
	      and de-serialize them. We should be able to develop a parametric
	      \texttt{map} definition as a list of tuples, just like how protobuf would
	      serialize them.
	\item \texttt{option}: This isn't an optional value, but rather top level
	      options used to set things like the \texttt{java} package. At the moment,
	      I'm not sure if or how changing values can impact protobuf compatibility.
\end{itemize}

A good completeness test might be encoding \texttt{descriptor.proto}, the proto
file which can encode other protobuf schema.

While this would provide a framework for building \fstar{} bindings for
protobuf, a richer representation for a protobuf schema for checking
compatibility at the wire level.

\subsection{Compatibility Checking with \fstar{} Types}

When a protobuf message is encoding, field names are stripped out in favor of
field numbers or field tags, which are manually defined in the protobuf
schema. Consider the two \texttt{proto} snippets below:

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    int32 bar = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{1cm}
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    int32 bar = 2;
}\end{lstlisting}
	\end{minipage}

	\caption[]{Example showing the importance of including proto field numbers in
		compatibility checking.}
\end{figure}

Any invocation of \texttt{protoc} will generate exactly the same
application-facing code, even though a message generated using the left proto
file will be parsed to an empty message by the right proto file.

This suggests that a more descriptive type is required. Rather than model the
struct that would be output by \texttt{protoc}, the source type for Pollux
should target the descriptor, as represented in the \texttt{.proto} file. Then,
in a manner similar to the \texttt{parse} function in Everparse, develop
something like this:

\begin{align*}
	\mathtt{Desc}               & : Type                                                                             \\
	\mathtt{Record}             & : Type \quad \text{Record corresponding to descriptor}                             \\
	\llbracket \cdot \rrbracket & : \mathtt{Desc} \rightarrow \mathtt{Record}                                        \\
	parse                       & : (d:\mathtt{Desc}) \rightarrow \mathtt{Bytes} \rightarrow option\
	\llbracket d \rrbracket                                                                                          \\
	serialize                   & : (d:\mathtt{Desc}) \rightarrow \llbracket d \rrbracket \rightarrow \mathtt{Bytes}
\end{align*}

Several shorthands arise from this, namely $parse_d : \mathtt{Bytes} \rightarrow option\
	\llbracket d \rrbracket$ for the partial application of a descriptor $d$ to
$parse$. The same thing can be done for $serialize_d : \llbracket d \rrbracket \rightarrow
	\mathtt{Bytes}$, which naturally to more traditional definitions of a parser and serializer.

\section{Protobuf Compatibility Questions}

As the \fstar{} development moves forward, it is beneficial to have a minimal
working example of a non-trivial compatibility question for protocol
buffers. There are several clearly trivial questions, like updating a
\texttt{int32} to an \texttt{int64} using the variable width integer encoding.

The protobuf encoding format is simple, which makes it challenging to construct
non-trivial compatibility question from small, trivial protobuf descriptions. As
an example, consider this example:

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Bar {
    string baz = 1;
}

message Foo {
    Bar bar = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{1cm}
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    string bar = 1;
}\end{lstlisting}
	\end{minipage}

	\caption[]{A compatibility question.}
\end{figure}

The data is correctly exposed in the string on the right but there will be
several junk bytes at the beginning that the application has to trim. As a
definitional question, should this be considered compatible? Where or not a
client application can detect and correct the contents of the \texttt{bar} field
is beyond the current scope of this project.

From an encoding prospective, a protobuf message is encoded (mostly) as a
sequence of id, tag and value tuples. The id is the field number encoded in the
proto descriptor, the tag marks the type of value to follow and finally the
value contains the content set by the encoding application. More specifically,
the tag is from the table below.

\begin{table}[H]
	\centering
	\begin{tabular}{cll}
		\toprule
		ID & Name            & Uses                                                 \\
		\midrule
		0  & \texttt{VARINT} & \texttt{int32}, \texttt{int64}, \texttt{uint32},
		\texttt{uint64}, \texttt{sint32}, \texttt{sint64},
		\texttt{bool}, \texttt{enum}                                                \\
		1  & \texttt{I64}    & \texttt{fixed64}, \texttt{sfixed64}, \texttt{double} \\
		2  & \texttt{LEN}    & \texttt{string}, \texttt{bytes}, embedded messages,
		packed repeated fields                                                      \\
		5  & \texttt{I32}    & \texttt{fixed32}, \texttt{sfixed32},
		\texttt{float}                                                              \\
		\bottomrule
	\end{tabular}

	\vspace{4mm}
	\caption[]{Description of \texttt{proto} tag values. Note that values 3 and
		4 correspond to \texttt{SGROUP} and \texttt{EGROUP}, tags used in protobuf
		version 2 and deprecated in protobuf 3 \autocite{Encoding}.}
\end{table}

It has finally become time for a sequence of definitions. A protobuf message
descriptor is comprised of a series of field descriptors, each of which has the
form \texttt{<<optional modifiers>> <<type>> <<field name>> = <<field
	number>>;}. During the encoding process, each field is encoded into an
encoding field, represented as tuple with 3 elements.

\begin{definition}
	An encoding field is a tuple $(id, tag, value)$ which encodes the field
	number, tag and value of one field of a proto descriptor.
\end{definition}

Just like how the message descriptor is a sequence of field descriptors, the
encoding type of a message is an ordered list of encoding fields.

\begin{definition}[Field Compatibility]
	For fields $f_1 = (id_1, tag_1, v_1)$ and $f_2 = (id_2, tag_2, v_2)$, $f_2$
	with descriptor $d_2$ is a compatible update to $f_1$ with descriptor $d_1$ if
	$id_1 = id_2$ and for all values $v_1$, $serialize_{d_1}\ f_1 = bs$ and there
	exists a $v_2$ such that $parse_{d_2}\ bs = v_2$ and $v_1 \sim v_2$ according to
	some relation $\sim$.
\end{definition}

One example of $\sim$ would be equality of mathematician integers represented with
different bit widths (i.e. 32-bit to 64-bit integers).

\begin{definition}[Message Compatibility]
	A protobuf message descriptor $d_2$ is a compatible update to $d_1$ if every
	field in $d_1$ has a compatible field in $d_2$.
\end{definition}

\printbibliography{}
\end{document}

% Local Variables:
% citar-bibliography: ("./pollux.bib")
% TeX-command-default: "Make"
% jinx-local-words: "Everparse proto protobuf protoc"
% End:
