\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{bbold}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{nicematrix}
\usepackage{mathpartir}
\usepackage{parskip}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usetikzlibrary{positioning,calc,snakes,automata,arrows.meta}
\usepackage{array}
\usepackage{multirow}
\usepackage{listings}
\usepackage{stmaryrd}
\usepackage[backend=biber,sorting=ynt]{biblatex}
\addbibresource{pollux.bib}
\author{Matt Schwennesen}
\title{Pollux: Protobuf Compatibility Checking}
\date{}
\hypersetup{
 pdfauthor={Matt Schwennesen},
 pdftitle={Pollux},
 pdfkeywords={},
 pdfsubject={},
 pdflang={English}
}

\lstdefinelanguage{proto}{
  keywords={message,double,float,int32,int64,uint32,uint64,sint32,sint64,fixed32,fixed64,sfixed32,sfixed64,bool,string,bytes,oneof,enum}
}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{pollux}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=pollux}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\newcommand{\fstar}{F$^\star$}
\renewcommand{\mod}{\;\%\;}

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}

Software has become an integral and integrated part of everyday life, and with
that comes a near constant stream of updates to various devices all around
us. Unfortunately, software updates are a frequent source of issues
\autocites{zhangUnderstandingDetectingSoftware2021}[][]{Gray1986WhyDC}, often
making it into production environments before the necessary edge cases are
triggered. We believe that techniques from formal verification can be applied
here to ensure update compatibility, which is ultimately what my project aims to
do. Unlike some previous work
\autocites{ajmaniModularSoftwareUpgrades2006}[][]{reitblattAbstractionsNetworkUpdate2012},
I aim to prove compatibility of software updates without imposing restrictions
about how the update itself is performed.

The most nebulous part of this goal is how to define ``compatible'', which I
break down into two major categories; data compatibility and operational
compatibility. Data compatibility aims to show that that the new version can
correctly interpret persistent state left behind by the pervious version while
operational compatibility reasons about behavior of invoking the same function
or feature in both versions of the software. This project is currently working
on data compatibility.

Imagine that you're a developer for a distributed system using \texttt{protobuf}
and \texttt{grpc} to communicate between nodes. A new version of
\texttt{protobuf} just came out and added a syntax for maps to the
protocol. When you used to need maps, you had to manually encode a list of
key-value pairs but now you can write \texttt{map<string, int32>} in the
\texttt{.proto} schema file. Is changing between these syntactic definitions a
compatible update? From a high level, they both model the same mathematical map,
which is the information the program is actually trying to communicate, so it
should be possible to define a compatible update this way. The specific answer
naturally depends on how the new \texttt{map} is encoded into binary, and in the
case of \texttt{protobuf} a \texttt{map} is syntactic sugar for a list of
key-value pairs so this is a valid update\footnote{There is slightly more to it
	than that, regarding some \texttt{optional} tags which might not present in
	the original definition but it is possible to define this update in a
	compatible way}. While this example is considering a rolling update to a
distributed system, the same question would apply to an application which writes
the serialized blob to disk and reads it when the application starts up again.

The notion of both types of maps encoding the same mathematical map leads us to
the idea of abstract state. When we're programming, we don't often think about
which bucket an element of a hashmap is in but rather we think at a higher
level. It is possible to conceptualize this formally, were we have some abstract
state and a concrete state which are linked with a predicate asserting that the
abstract state is captured by the concrete state. This is the same relationship
we see in proofs of functional correctness, were the specification of a function
corresponds to the abstract state and the implementation to the concrete
state. This relationship will serve as the theoretical backing to the project.

More practically, consider an RPC or other serialization library such as
Protobuf or Cap'n'Proto. Updates to RPC schema are an excellent place to start
exploring notions of data compatibility since RPC schema are separated from the
source code that operates on them, are highly structured serialized formats and
are designed to allow the schema to evolve though updates. Tools already exist
which can report on if two versions of a schema file are compatible, these tools
operate using ad-hoc notions of compatibility asserting a specification or
verifying the implementation. Using \fstar{}, I plan to verify a compatibility
checker for one of these libraries before expanding the project to consider a
wider range of data formats. It should also be possible to include some analysis
of the source code operating on the state to prove more invasive updates, or
even the proofs of verified pieces of software, although that is a goal for a
longer timescale.

\section{Protobuf Primer}

Protobuf, or protocol buffers, is a message description language and
serialization protocol developed by Google. It is similar to other descriptive
data formats like ASN1, and is commonly used as part of the gRPC remote
procedure call system. Protobuf and gRPC are one of the most popular
serialization formats.

\subsection{The \texttt{proto} Language}

A high-level protobuf message description is written in a \texttt{.proto} file
in the \texttt{proto} language. Each message is described by a \emph{message
	descriptor} which declares each field and its corresponding type.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 results_per_page = 3;
}\end{lstlisting}

	\caption{Example protobuf message description from the protobuf documentation~\cite{LanguageGuideProto}.}
	\label{fig:proto-ex}
\end{figure}

In figure~\ref{fig:proto-ex}, a message called \texttt{SearchRequest} is
declared. This message contains three fields, the query, page number to display
and number of results per page. Each field is assigned a \emph{field number},
which is used in the serialized version of the message. Each field number must
be unique, as they are used to identify the corresponding binary blob in the
encoded message. A \texttt{.proto} file can define multiple messages, and
messages can be embedded in another by using the name of that message as the
type of a field in the other message.

Each field is defined with a type, with the primitive types summarized in
table~\ref{tbl:proto-prim}.

\begin{table}[H]
	\centering
	\begin{tabular}{cl}
		\toprule
		Type              & Notes                                                    \\
		\midrule
		\texttt{double}   & Standard 64 bit double-precision floating point
		number.                                                                      \\
		\texttt{float}    & Standard 32 bit floating point number.                   \\
		\texttt{int32}    & Variable-length signed 32 bit integer.                   \\
		\texttt{int64}    & Variable-length signed 64 bit integer.                   \\
		\texttt{uint32}   & Variable-length unsigned 32 bit integer.                 \\
		\texttt{uint64}   & Variable-length unsigned 64 bit integer.                 \\
		\texttt{sint32}   & Variable-length 32 bit integer with more
		efficient encoding for negative numbers.                                     \\
		\texttt{sint64}   & Variable-length 64 bit integer with more efficient
		encoding for negative numbers.                                               \\
		\texttt{fixed32}  & Four byte integer, more efficient than \texttt{uint32}
		for values greater than $2^{38}$.                                            \\
		\texttt{fixed64}  & Eight byte integer, more efficient than \texttt{uint64}
		for values greater than $2^{56}$.                                            \\
		\texttt{sfixed32} & Four byte signed integer.                                \\
		\texttt{sfixed64} & Eight byte signed integer.                               \\
		\texttt{bool}     & Boolean, encoded in one byte.                            \\
		\texttt{string}   & A string of UFT-8 or 7-bit ASCII characters shorter than
		$2^{32}$ bytes.                                                              \\
		\texttt{bytes}    & An arbitrary sequence of less than $2^{32}$ bytes.       \\
		\bottomrule
	\end{tabular}
	\caption{Summary of the protobuf scalar values.}
	\label{tbl:proto-prim}
\end{table}

Any protobuf field can marked as \texttt{optional} or \texttt{repeated}. An
\texttt{optional} field can be either set to a value or omitted from the
message. Similarly, a \texttt{repeated} field can be repeated in the message
zero or more times, functioning as a list now. A field without a cardinality
modifier is known as an implicit field and can also be omitted from a message,
just like an \texttt{optional} field. During de-serialization, a type-specific
default value will be used (this is consistent with accessing the value of an
unset \texttt{optional} field). The only difference between an implicit field
and \texttt{optional} field is that the optional field can differentiate between
an unset field and field set to the default value by providing a method which
reports if the \texttt{optional} field is set.

Since the protobuf language is designed to be both forward and backwards
compatible, it is possible to remove or add fields. When removing fields, it is
encouraged to reserve the field number to prevent reuse of the field in a
future. It is also possible to reserved field names, which isn't important for
the protobuf binary encoding format, this is important if you're using the JSON
encoding of a protobuf message.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

message Foo {
  reserved 2, 15, 9 to 11;
  reserved "foo", "bar";
}\end{lstlisting}

	\caption{Example protobuf message using reserved fields~\cite{LanguageGuideProto}.}
	\label{fig:proto-reserved}
\end{figure}

Protobuf also supports enumerations, for when a field may only take a finite
number of values. Due to how default values work in protobuf, enums are required
to have a value defined with a value zero and it should have either
``UNSPECIFIED'' or ``UNKNOWN'' since that will be the default value.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

enum {
  CORPUS_UNSPECIFIED = 0;
  CORPUS_UNIVERSAL = 1;
  CORPUS_WEB = 2;
  CORPUS_IMAGES = 3;
  CORPUS_LOCAL = 4;
  CORPUS_NEWS = 5;
  CORPUS_PRODUCTS = 6;
  CORPUS_VIDEO = 7;
}

message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 results_per_page = 3;
  Corpus corpus = 4;
}\end{lstlisting}

	\caption{Example protobuf message using an enum~\cite{LanguageGuideProto}.}
	\label{fig:proto-enum}
\end{figure}

There are two other features which need to be modeled, \texttt{map} and
\texttt{oneof}. Maps can be thought of like a standard map in \texttt{go} or a
\texttt{dict} in \texttt{python}. From a serialization perspective, this is an
unordered map that could be modeled as a list of key-value pairs. A \texttt{map}
field cannot be \texttt{repeated}, and repeated keys have the last occurrence
win, as is standard with protobuf (see Section~\ref{sec:proto-enc}).

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.43\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
  map<int32, string> map_field = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}[bt]{0.48\textwidth}
		\begin{lstlisting}[language=proto]
message MapFieldEntry {
  int32 key = 1;
  string value = 2;
}

message Foo {
  repeated MapFieldEntry map_field = 1;
}\end{lstlisting}
	\end{minipage}

	\caption{Example of \texttt{map} syntax and the corresponding
		wire-equivalent syntax~\cite{LanguageGuideProto}.}
	\label{fig:proto-map}
\end{figure}

The final important feature of the proto language is a \texttt{oneof} field,
which operates like a union in \texttt{C}. Setting any member of the
\texttt{oneof} will clear any previously set field. A \texttt{oneof} field
cannot contain a \texttt{repeated} field or a \texttt{map} and the
\texttt{oneof} field itself cannot be \texttt{repeated}.

\begin{figure}[H]
	\begin{lstlisting}[language=proto]
syntax = "proto3";

message User {
  oneof user_id {
    string email = 4;
    int32 phone = 2;
  }
}\end{lstlisting}

	\caption{Example protobuf message using an enum~\cite{LanguageGuideProto}.}
	\label{fig:proto-enum2}
\end{figure}

\subsection{The Protobuf Encoding}\label{sec:proto-enc}

Complimenting the Protobuf schema language is the protobuf encoding format. All
valid protobuf messages written the the proto language will be eventually
encoding into a binary blob in respecting this encoding.

\subsubsection{Variable-Length Integers}

The heart of the protobuf encoding is the variable-length integer encoding,
which represents a 32 or 64 bit integer in between one and ten bytes. The
encoding format itself is relatively simple. The bits of the integer to be
encoded are grouped into bundles of seven. The bundles are ordered in little
endian order with the first bit of each byte set to one if the next byte is also
part of the same integer.

For example, consider the number 163, or 10100011 in binary. Splitting these
into bundles of seven bits produces ``0000001'' and ``0100011''. These are
reordered into little endian order, ``0000001'' then ``0100011''. Finally set
the continuation bit on the first byte of the encoding and concatenate them
together to get ``10000010100011''.

\subsubsection{Field Identifiers}

A message is serialized as a sequence of fields, in any order. Each field is
encoded as a \emph{tag-length value} structure, although not ever field has a
dedicated length in the encoding. The tag consists of the field number as
defined in the \texttt{proto} file, encoded in the variable length integer
encoding discussed above. However, the tag isn't just the field number, it also
includes information on the type of payload as detailed in
Table~\ref{tab:tags}. The wire type is stored in the last three bits of the
field number varint, effectively encoded as \texttt{(field\_number << 3) |
  wire\_type}. 

\subsubsection{Length Delimited Fields}

Nested messages, string and bytes are encoded with their payload length in bytes
as a varint just after the tag. Despite being variable-length, varint fields
aren't encoded with an explicit length since checking the length of the varint
doesn't provide significance benefits when the maximum length varint is only 10
bytes. Lengths are encoded in bytes so that deserializers can skip large unknown
fields efficiently and know when exactly the unknown field ends.

All variable length fields except varints are prefixed with a length and then
the rest of the payload, with nested messages simply beginning with the tag for
their first field and continuing from there. Since nested messages include the
complete field encodings, it is important for the parser to be able to skip an
unknown field exactly rather than incorrectly thinking a nested unknown field is
setting field in the parent message.

\subsubsection{Signed Integers}

The standard varint encoding is only for unsigned integers. Signed integers can
be encoded in both \texttt{int32} and \texttt{int64} or \texttt{sint32} and
\texttt{sint64} field types. The \texttt{intN} types uses standard two's
complement to encode negative integers. However, since a negative two's
compliment integer always has the sign bit set, any negative number will be
encoded in a maximum length varint 10 bytes long.

On the other hand, the \texttt{sintN} types use a ``ZigZag'' encoding to more
efficiently represent smaller negative numbers~\cite{Encoding}. In this
encoding, a positive value $p$ is encoded as $2 \times p$ while a negative value $n$
is encoded as $2 \times |n| - 1$. Using bit-shift operators, this would be
\verb|(n << 1) ^ (n >> 31)| or the naturally extended 64-bit version.

\begin{table}[htbp]
  \centering
  \begin{tabular}{cc}
    \toprule
    Signed Original & Zig-Zag Encoding \\
    \midrule
    0 & 0 \\
    -1 & 1 \\
    1 & 2 \\
    -2 & 3 \\
    \vdots & \vdots \\
    \bottomrule
  \end{tabular}
\caption{Some zig zag encoded integers}
\label{tab:zigzag}
\end{table}

While the zig-zag encoding does save space for smaller negative values in
particular, it does increase the amount of space required to encode large
positive values, which is why it isn't the standard encoding for signed
integers. 

\section{Abstract State Schema}

In software verification, an implementation is always verified against some
specification of correct behavior. This is implicitly part of
\texttt{protobuf} too; when a message is serialized and transmitted over the
network, the developer expects that the same structure can be recreated by the
receiving program. Refactoring this intuition to be more directly applicable to
the standard verification setup, one could argue that the binary wire blob
serves as an ``implementation'' of the structure or object in the host
language (be it \texttt{go}, \texttt{java}, \texttt{python} or any other
language with \texttt{protobuf} bindings) which itself is generated by
\texttt{protoc} and is an ``implementation'' of the proto schema defined in a
\texttt{.proto} file. This stack defines the \texttt{.proto} file as the top
level source of truth, but this can be modeled by the high level host language
in most cases. From the prospective of an application developer, the host
language representation is the most important, since this is where the schema
structure interacts with other parts of the host application.

The definition of a \texttt{proto} message can be modeled by an \fstar{} type,
in a way similar to structures created by \texttt{protoc} for other host
languages. This is not fully sufficient though, since the representation
exposed by \texttt{protoc} isn't enough to fully reason about compatibility.

\subsection{\fstar{} Proto Struct Representation}

Some parts of the translation from a \texttt{proto} message
\autocite{LanguageGuideProto} into \fstar{} is simple. For example, a 64 bit
unsigned integer can be represented with a \texttt{UInt64} from the \fstar{}
machine integer library. Likewise a message can be represented by a record
\autocite{swamy2023proof}. Not everything cleanly generalizes
though. Specifically, it is unclear to how represent some primitive types such
as \texttt{float}, \texttt{double} or \texttt{string}.

\begin{table}[H]
	\centering
	\begin{tabular}{ll}
		\toprule
		\texttt{proto} Feature & \fstar{} Representation   \\
		\midrule
		Integers               & Machine Integer Library   \\
		\texttt{string}        & \texttt{string}           \\
		\texttt{message}       & Record                    \\
		\texttt{optional}      & \texttt{option}           \\
		\texttt{repeated}      & \texttt{list}             \\
		\texttt{map}           & ???                       \\
		\texttt{bool}          & \texttt{bool}             \\
		\texttt{bytes}         & \texttt{Seq.seq UInt8.t}  \\
		\texttt{enum}          & Inductive type            \\
		\texttt{oneof}         & Inductive type (Sum type) \\
		\texttt{option}        & ???                       \\
		\bottomrule
	\end{tabular}

	\vspace{4mm}
	\caption[]{\texttt{proto} language features and corresponding \fstar{} representations.}
\end{table}

Notice that there are several unknown representations in the table. Below is
some commentary on these features:

\begin{itemize}
	\item \texttt{map}: As far as I can tell, \fstar{} doesn't have maps, however
	      this \fstar{} development doesn't \emph{need} maps beyond a way to serialize
	      and de-serialize them. We should be able to develop a parametric
	      \texttt{map} definition as a list of tuples, just like how protobuf would
	      serialize them.
	\item \texttt{option}: This isn't an optional value, but rather top level
	      options used to set things like the \texttt{java} package. At the moment,
	      I'm not sure if or how changing values can impact protobuf compatibility.
\end{itemize}

A good completeness test might be encoding \texttt{descriptor.proto}, the proto
file which can encode other protobuf schema.

While this would provide a framework for building \fstar{} bindings for
protobuf, a richer representation for a protobuf schema for checking
compatibility at the wire level.

\subsection{Compatibility Checking with \fstar{} Types}

When a protobuf message is encoding, field names are stripped out in favor of
field numbers or field tags, which are manually defined in the protobuf
schema. Consider the two \texttt{proto} snippets below:

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    int32 bar = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{1cm}
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    int32 bar = 2;
}\end{lstlisting}
	\end{minipage}

	\caption[]{Example showing the importance of including proto field numbers in
		compatibility checking.}
\end{figure}

Any invocation of \texttt{protoc} will generate exactly the same
application-facing code, even though a message generated using the left proto
file will be parsed to an empty message by the right proto file.

This suggests that a more descriptive type is required. Rather than model the
struct that would be output by \texttt{protoc}, the source type for Pollux
should target the descriptor, as represented in the \texttt{.proto} file. Then,
in a manner similar to the \texttt{parse} function in Everparse, develop
something like this:

\begin{align*}
	\mathtt{Desc}               & : Type                                                                             \\
	\mathtt{Record}             & : Type \quad \text{Record corresponding to descriptor}                             \\
	\llbracket \cdot \rrbracket & : \mathtt{Desc} \rightarrow \mathtt{Record}                                        \\
	parse                       & : (d:\mathtt{Desc}) \rightarrow \mathtt{Bytes} \rightarrow option\
	\llbracket d \rrbracket                                                                                          \\
	serialize                   & : (d:\mathtt{Desc}) \rightarrow \llbracket d \rrbracket \rightarrow \mathtt{Bytes}
\end{align*}

Several shorthands arise from this, namely $parse_d : \mathtt{Bytes} \rightarrow option\
	\llbracket d \rrbracket$ for the partial application of a descriptor $d$ to
$parse$. The same thing can be done for $serialize_d : \llbracket d \rrbracket \rightarrow
	\mathtt{Bytes}$, which naturally to more traditional definitions of a parser and serializer.

\section{Protobuf Compatibility Questions}

As the \fstar{} development moves forward, it is beneficial to have a minimal
working example of a non-trivial compatibility question for protocol
buffers. There are several clearly trivial questions, like updating a
\texttt{int32} to an \texttt{int64} using the variable width integer encoding.

The protobuf encoding format is simple, which makes it challenging to construct
non-trivial compatibility question from small, trivial protobuf descriptions. As
an example, consider this example:

\begin{figure}[H]
	\centering
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Bar {
    string baz = 1;
}

message Foo {
    Bar bar = 1;
}\end{lstlisting}
	\end{minipage}
	\hspace{1cm}
	\begin{minipage}[bt]{0.4\textwidth}
		\begin{lstlisting}[language=proto]
message Foo {
    string bar = 1;
}\end{lstlisting}
	\end{minipage}

	\caption[]{A compatibility question.}
\end{figure}

The data is correctly exposed in the string on the right but there will be
several junk bytes at the beginning that the application has to trim. As a
definitional question, should this be considered compatible? Where or not a
client application can detect and correct the contents of the \texttt{bar} field
is beyond the current scope of this project.

From an encoding prospective, a protobuf message is encoded (mostly) as a
sequence of id, tag and value tuples. The id is the field number encoded in the
proto descriptor, the tag marks the type of value to follow and finally the
value contains the content set by the encoding application. More specifically,
the tag is from the table below.

\begin{table}[H]
	\centering
	\begin{tabular}{cll}
		\toprule
		ID & Name            & Uses                                                 \\
		\midrule
		0  & \texttt{VARINT} & \texttt{int32}, \texttt{int64}, \texttt{uint32},
		\texttt{uint64}, \texttt{sint32}, \texttt{sint64},
		\texttt{bool}, \texttt{enum}                                                \\
		1  & \texttt{I64}    & \texttt{fixed64}, \texttt{sfixed64}, \texttt{double} \\
		2  & \texttt{LEN}    & \texttt{string}, \texttt{bytes}, embedded messages,
		packed repeated fields                                                      \\
		5  & \texttt{I32}    & \texttt{fixed32}, \texttt{sfixed32},
		\texttt{float}                                                              \\
		\bottomrule
	\end{tabular}

	\vspace{4mm}
	\caption[]{Description of \texttt{proto} tag values. Note that values 3 and
		4 correspond to \texttt{SGROUP} and \texttt{EGROUP}, tags used in protobuf
		version 2 and deprecated in protobuf 3 \autocite{Encoding}.}
    \label{tab:tags}
\end{table}

It has finally become time for a sequence of definitions. A protobuf message
descriptor is comprised of a series of field descriptors, each of which has the
form \texttt{<<optional modifiers>> <<type>> <<field name>> = <<field
	number>>;}. During the encoding process, each field is encoded into an
encoding field, represented as tuple with 3 elements.
\\
\begin{definition}[Encoding Field]
	An encoding field is a tuple $(id, tag, value)$ which encodes the field
	number, tag and value of one field of a proto descriptor.
\end{definition}

Just like how the message descriptor is a sequence of field descriptors, the
encoding type of a message is an ordered list of encoding fields.
\\
\begin{definition}[Field Compatibility]
	For fields $f_1 = (id_1, tag_1, v_1)$ and $f_2 = (id_2, tag_2, v_2)$, $f_2$
	with descriptor $d_2$ is a compatible update to $f_1$ with descriptor $d_1$ if
	$id_1 = id_2$ and for all values $v_1$, $serialize_{d_1}\ f_1 = bs$ and there
	exists a $v_2$ such that $parse_{d_2}\ bs = v_2$ and $v_1 \prec v_2$ according to
	some relation $\prec$.
\end{definition}

An example value relation is given in Section~\ref{sec:val-rel}.
\\
\begin{definition}[Message Compatibility]
	A protobuf message descriptor $d_2$ is a compatible update to $d_1$ if every
	field in $d_1$ has a compatible field in $d_2$.
\end{definition}

Formally speaking, the message compatibility relation ($\preceq$) is given in
Section~\ref{sec:comp-rel}.
\\
\begin{definition}[Tag Function]
  The tag function $tag : \text{Type} \rightarrow \text{Tag}$ maps the \texttt{proto}
  level type to the corresponding tag as listed in Table~\ref{tab:tags}.
\end{definition}

\section{Compatibility Relation}~\label{sec:comp-rel}

The descriptor compatibility relation $\preceq$ is defined as below. It is inspired by
a subtyping relation over tuples $d = (n, \tau, m, v)$ where $n$ is a field number,
$\tau$ a type at the proto level, $m$ a field modifier (one of \texttt{IMPLICIT},
\texttt{OPTIONAL} or \texttt{REPEATED}) and $v$ is a payload for the field.
This relation is defined in terms of another relation $\prec$ defined on just values
which allow for modification of the values during the deserialization process.

\subsection{Basic Rules}

A compatibility relation is both reflexive and transitive, although it is
neither symmetric nor anti-symmetric.  

\begin{mathpar}
   \infer[Refl]{ }{(n, \tau, m, v) \preceq (n, \tau, m, v)}

   \infer[Trans]{(n_1, \tau_1, m_1, v_1) \preceq (n_2, \tau_2, m_2, n_3) \\ (n_2,
     \tau_2, m_2, n_2) \preceq (n_3, \tau_3, m_3, v_3)}{(n_1, \tau_1, m_1, v_1) \preceq (n_3, \tau_3,
     m_3, v_3)}
\end{mathpar}

\subsection{Modifier Rules}

We should be able to change field modifiers using these rules.

\begin{mathpar}
    \infer[Add-Opt]{ }{(id, \tau, \mathtt{IMPLICIT}, v) \sim (id, \tau,
      \mathtt{OPTIONAL}, v)}

    {\color{red}
    \infer[Rm-Opt]{ }{(id, \tau, \mathtt{OPTIONAL}, v) \sim (id, \tau,
      \mathtt{IMPLICIT}, v)}}

    \infer[Add-Rep]{ }{(id, \tau, \mathtt{IMPLICIT}, v) \sim (id, \tau,
      \mathtt{REPEATED}, v)} 

\end{mathpar}

\subsection{Type Rules}

Since the descriptor compatibility relation uses the value relation, it is
possible to update the proto-level types of the descriptors to any other type
related by $\prec$.

\begin{mathpar}
  \infer[Type-Chg]{v_1 : \tau_1 \prec v_2 : \tau_2}{(id, \tau_1, m, v_1) \preceq (id, \tau_2, m, v_2)}
\end{mathpar}

\subsection{Message Rules}

Messages are similar to records in \fstar{} or other functional languages and
message compatibility can take the same two routes that record subtyping can:
width compatibility and depth compatibility. In width compatibility updates, new
fields are added to the message, making the updated version a strict superset of
the original message. In depth compatibility updates, the number of fields
remains the same, but other attributes of the field are updated compatible ways.

\begin{mathpar}
  \infer[Msg-Width]{n \le m}{\{d_1, \dots, d_n\} \preceq \{d_1, \dots, d_n,
    \dots, d_m\}}

  \infer[Msg-Depth]{d_1 \preceq d_1' \\ \cdots \\ d_n \preceq d_n'}{\{d_1,
    \dots, d_n\} \preceq \{d_1', \dots, d_n'\}}

  \hspace{1cm}
  \infer[Msg-Field]{\mathtt{msg}_1 \preceq \mathtt{msg}_2}{(id, \mathtt{msg}_1,
    m, v) \preceq (id, \mathtt{msg}_2, m, v)} 
\end{mathpar}

\subsection{Enum Rules}

The enum rules are similar to the message rules at least in the sense that enums
can be updated to have new fields without issue. However, when considering
removing fields from an enum definition, it is important to know if the enum is
treated as open or closed~\cite{EnumBehavior}. By default, \texttt{proto3}
treats all enums as open, so it should be possible to write a rule allowing for
the deletion of an enum field. This naturally aligns with how \texttt{go}
handles enums, since it doesn't support closed enums. Since protobuf enums don't
contain extra information, no depth rules is allowed.

\begin{mathpar}
  \infer[Enum-Width]{n \le m}{\{id_1, \dots, id_n\} \preceq \{id_1, \dots, id_n,
    \dots, id_m\}}

  \infer[Enum-Field]{\mathtt{enum}_1 \preceq \mathtt{enum}_2}{(id, \mathtt{enum}_1,
    m, v) \preceq (id, \mathtt{enum}_2, m, v)} 
\end{mathpar}

\subsection{Compatibility Theorem}

The main field-level compatibility theorem is stated below.
\\
\begin{theorem}[Field Compatibility]
  If $d_1 \preceq d_2$ then for all binary strings $bs$ such that
  $\mathtt{parse}_{d_1}\ bs = \mathtt{Some}\ v_1$, $\mathtt{parse}_{d_2}\ bs =
  \mathtt{Some}\ v_2$ and $v_1 \prec v_2$
\end{theorem}

\section{Value Relation}~\label{sec:val-rel}

The value relation operates on values expressed at a specific type. This
relation shows both what types can be changed in the \textsc{Type-Chg} rules of
$\preceq$. 

\subsection{String \& Byte Rules}

It is possible to convert been the \texttt{string} and \texttt{bytes} types,
although the host program may have to deal with control character bytes being
present in the resulting string.

\begin{mathpar}
  \infer[Str-Byte]{ }{v : \mathtt{string} \prec v : \mathtt{bytes}}

  \infer[Byte-Str]{ }{v : \mathtt{bytes} \prec v : \mathtt{string}}
\end{mathpar}

\subsection{Integer Rules}

As a shorthand, statements like \texttt{uint[n]} represent a variable length
integer encoding into $n$ bits. In order to result in valid protobuf types, $n \in
\{32, 64\}$. The change width rules are designed to allow for both integer
promotion and demotion while the rest of the rules express what happens when
converting between integers using different encoding types for negative numbers.

With regard to the \textsc{Int-Chg-W} rule, I was originally concerned about
negative 32 bit numbers being encoded into 5 bytes for a varint encoding and
then becoming positive if parsed into 64 bit integer, but protobuf handles this
by writing all negative \texttt{int[n]} into a full 10 bytes. 

It is also worth noting that the expression $v : \tau$ actually refers to a value
of type $\llbracket \tau \rrbracket$ with a label of the type. All of the integer
types have $\llbracket \cdot \rrbracket = \mathbb{Z}$, which allows for the free conversion
between integer types. Finally, $\%$ refers to modulus using truncated division,
so $-1 \mod 2 = -1$ and the rules are written with truncated integer division. 

\begin{mathpar}
  \infer[Uint-Chg-W]{v_2 = v_1 \mod 2^m}{v_1 : \mathtt{uint[n]} \prec v_2 : \mathtt{uint[m]}}

  % This rules works ONLY if implicitly sign-extend numbers during integer promotion
  \infer[Int-Chg-W]{v_2 = (v_1 \mod 2^m - 2^m \times \mathbb{1}[v_1 \ge 2^{m-1}]) \mod
    2^m}{v_1 : \mathtt{int[n]} \prec v_2 : \mathtt{int[m]}}

  \infer[Sint-Chg-W]{v_2 = v_1 \mod 2^{m-1}}{v_1 : \mathtt{sint[n]} \prec v_2 : \mathtt{sint[m]}}

  \infer[Uint-Int]{v_2 = v_1 - 2^n \times \mathbb{1}[v_1 \ge 2^{n-1}]}{v_1 :
    \mathtt{uint[n]} \prec v_2 : \mathtt{int[n]}}

  \infer[Int-Uint]{v_2 = v_1 + 2^n \times \mathbb{1}[v_1 < 0]}{v_1 :
    \mathtt{int[n]} \prec v_2 : \mathtt{uint[n]}} 

  \infer[Uint-Sint]{v_2 = (-1)^{v_1} \times \left( \frac{v_1}{2} \right) - (v_1 \%
    2)}{v_1 : \mathtt{uint[n]} \prec v_2 : \mathtt{sint[n]}} 

  \infer[Sint-Uint]{v_2 = 2 \times |v_1| - \mathbb{1}[v_1 < 0]}{v_1 :
    \mathtt{sint[n]} \prec v_2 : \mathtt{uint[n]}} 

  \infer[Int-Sint]
  {
    v_2 = \left\{
      {\begin{array}{l@{\quad:\quad}l}
        (-1)^{v_1} \times \left( \frac{v_1}{2} \right) - (v_1 \mod 2) & \text{if } v_1 \ge 0 \\
        (-1)^{v_1} \times \left(v_1 + 2^{n-1} - \frac{v_1}{2} \right) + (v_1 \mod 2) & \text{otherwise}
      \end{array}}\right.
  }
  {v_1 : \mathtt{int[n]} \prec v_2 : \mathtt{sint[n]}}

  \infer[Sint-Int]
  {
    v_2 = \left\{
      {\begin{array}{l@{\quad:\quad}l}
        2 \times |v_1| - \mathbb{1}[v_1 < 0] & \text{if } -2^{n-2} \le v_1 < 2^{n-2} \\
        |2 \times v_1 - 2^{n-1}| - 2^{n-1} - \mathbb{1}[v_1 < 2^{n-2}] & \text{otherwise}
      \end{array}}\right.
  }
  {v_1 : \mathtt{sint[n]} \prec v_2 : \mathtt{int[n]}}
\end{mathpar}

\subsection{Boolean Rules}

\begin{mathpar}
  \infer[Uint-Bool]
  {
    v_2 = \left\{
      {\begin{array}{l@{\quad:\quad}l}
        \mathtt{false} & \text{if } v_1 = 0 \\
        \mathtt{true} & \text{otherwise}
      \end{array}}\right.
  }
  {v_1 : \mathtt{uint[n]} \prec v_2 : \mathtt{bool}}

  \infer[Bool-Uint]{v_2 = \mathbb{1}[v_1]}{v_1 : \mathtt{bool} \prec v_2 :
    \mathtt{uint[n]}} 

  \infer[Int-Bool]
  {
    v_2 = \left\{
      {\begin{array}{l@{\quad:\quad}l}
        \mathtt{false} & \text{if } v_1 = 0 \\
        \mathtt{true} & \text{otherwise}
      \end{array}}\right.
  }
  {v_1 : \mathtt{int[n]} \prec v_2 : \mathtt{bool}}

  \infer[Bool-Int]{v_2 = \mathbb{1}[v_1]}{v_1 : \mathtt{bool} \prec v_2 :
    \mathtt{int[n]}} 

  \infer[Sint-Bool]
  {
    v_2 = \left\{
      {\begin{array}{l@{\quad:\quad}l}
        \mathtt{false} & \text{if } v_1 = 0 \\
        \mathtt{true} & \text{otherwise}
      \end{array}}\right.
  }
  {v_1 : \mathtt{sint[n]} \prec v_2 : \mathtt{bool}}

  \infer[Bool-Sint]{v_2 = -\mathbb{1}[v_1]}{v_1 : \mathtt{bool} \prec v_2 :
    \mathtt{sint[n]}} 
\end{mathpar}

\subsection{Enum Rules}

TODO: I'd like $\llbracket \mathtt{enum} \rrbracket$ to be an inductive data
type, so the value of a specific enum would be a constructor, but when cast to
an integer would result in the field number (except for \texttt{sint}'s). The
presentation of the formal rules is just a matter of notation, I think.

\printbibliography{}
\end{document}

% Local Variables:
% citar-bibliography: ("./pollux.bib")
% TeX-command-default: "Make"
% jinx-local-words: "Everparse proto protobuf protoc"
% End:
